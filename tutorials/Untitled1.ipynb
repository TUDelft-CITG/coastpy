{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../src\")\n",
    "\n",
    "from coastmonitor.io.drive_config import configure_instance\n",
    "\n",
    "configure_instance(branch=\"dev\")\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import dask\n",
    "\n",
    "dask.config.set({\"dataframe.query-planning\": False})\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import dask_geopandas\n",
    "import fsspec\n",
    "from dotenv import load_dotenv\n",
    "from geopandas.array import GeometryDtype\n",
    "\n",
    "from coastpy.geo.quadtiles_utils import add_geo_columns\n",
    "\n",
    "VERSION = \"2024-12-21\"\n",
    "OUT_BASE_URI = f\"az://shorelinemonitor-raw-series/release/{VERSION}\"\n",
    "TMP_BASE_URI = OUT_BASE_URI.replace(\"az://\", \"az://tmp/\")\n",
    "LOG_BASE_URI = OUT_BASE_URI.replace(\"az://\", \"az://log/\")\n",
    "\n",
    "TMP_OBS_PART_URI = f\"{TMP_BASE_URI}/obs/part\"\n",
    "TMP_OBS_BOX_URI = f\"{TMP_BASE_URI}/obs/box\"\n",
    "\n",
    "LOG_OBS_PART_URI = f\"{LOG_BASE_URI}/obs/part\"\n",
    "LOG_OBS_BOX_URI = f\"{LOG_BASE_URI}/obs/box\"\n",
    "LOG_FAILED_TRANSECTS_URI = f\"{LOG_BASE_URI}/obs/failed-transects\"\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "sas_token = os.getenv(\"AZURE_STORAGE_SAS_TOKEN\")\n",
    "account_name = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "storage_options = {\"account_name\": account_name, \"credential\": sas_token}\n",
    "\n",
    "logging.getLogger(\"azure\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "def to_out_href(base_href, filename):\n",
    "    return base_href + filename\n",
    "\n",
    "\n",
    "DTYPES = {\n",
    "    # Identifiers\n",
    "    \"transect_id\": str,  # Unique identifier for the transect\n",
    "    \"shoreline_id\": str,  # Unique identifier for the shoreline observation\n",
    "    # Date/Time\n",
    "    \"datetime\": \"datetime64[ns]\",  # Timestamp of the shoreline observation\n",
    "    # Positional Attributes\n",
    "    \"geometry\": GeometryDtype(),  # Shoreline geometry as a LineString\n",
    "    \"lon\": \"float32\",  # Longitude of the shoreline position\n",
    "    \"lat\": \"float32\",  # Latitude of the shoreline position\n",
    "    \"chainage\": \"float32\",  # Distance along the transect\n",
    "    \"utm_epsg\": int,  # EPSG code for the UTM zone\n",
    "    \"transect_lon\": \"float32\",  # Longitude of the transect origin\n",
    "    \"transect_lat\": \"float32\",  # Latitude of the transect origin\n",
    "    \"quadkey\": str,  # Quadkey for spatial indexing\n",
    "    \"bbox\": object,  # Bounding box for spatial indexing\n",
    "    # Derived Metrics\n",
    "    \"sinuosity\": \"float32\",  # Ratio of actual length to straight-line length\n",
    "    \"self_intersection_density\": \"float32\",  # Density of self-intersections per unit length\n",
    "    \"fractal_dimension\": \"float32\",  # Fractal complexity of the shoreline\n",
    "    \"is_shoal\": bool,  # Flag indicating if the shoreline is over a shoal\n",
    "    # Observation Attributes\n",
    "    \"obs_group\": int,  # Group identifier for aggregated observations\n",
    "    \"obs_group_stdev\": \"float32\",  # Standard deviation within the observation group\n",
    "    \"obs_group_range\": \"float32\",  # Range of values within the observation group\n",
    "    \"obs_group_count\": int,  # Count of observations within the group\n",
    "    \"obs_is_qa\": bool,  # Flag indicating whether the observation passed QA\n",
    "    # Metrics from source data\n",
    "    \"otsu_threshold\": \"float32\",  # Otsu threshold value for water detection\n",
    "    \"otsu_separability\": \"float32\",  # Otsu separability score\n",
    "    \"composite:image_id\": \"string\",  # Identifier for the composite image\n",
    "    \"composite:start_datetime\": \"datetime64[ns, UTC]\",  # Start of composite period\n",
    "    \"composite:end_datetime\": \"datetime64[ns, UTC]\",  # End of composite period\n",
    "    \"composite:determination_datetimes\": \"object\",  # Array of datetime objects\n",
    "    \"composite:cloud_cover\": \"object\",  # Array of cloud cover percentages\n",
    "}\n",
    "\n",
    "\n",
    "def group_files_by_box(filenames):\n",
    "    \"\"\"\n",
    "    Group filenames by box numbers. Handles filenames that include optional parts.\n",
    "\n",
    "    Args:\n",
    "        filenames (list): List of filenames to be grouped.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with box numbers as keys and lists of filenames as values.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"(box_\\d+)_.*\\.parquet\")\n",
    "\n",
    "    # pattern = re.compile(r\"(box_\\d+)_*.parquet\")\n",
    "    grouped_files = defaultdict(list)\n",
    "\n",
    "    for filename in filenames:\n",
    "        match = pattern.search(filename)\n",
    "        if match:\n",
    "            grouped_files[match.group(1)].append(filename)\n",
    "\n",
    "    return dict(grouped_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from distributed import Client\n",
    "\n",
    "# client = Client()\n",
    "# print(client.dashboard_link)\n",
    "\n",
    "fs = fsspec.filesystem(\"az\", **storage_options)\n",
    "files = fs.ls(TMP_OBS_PART_URI)\n",
    "files = group_files_by_box(files)\n",
    "\n",
    "# NOTE list the already processed files in the box directory\n",
    "try:\n",
    "    processed_files = fs.ls(TMP_OBS_BOX_URI)\n",
    "    box_pattern = re.compile(r\"(box_\\d+).parquet\")\n",
    "\n",
    "    processed_boxes = []\n",
    "    for filename in processed_files:\n",
    "        match = box_pattern.search(filename)\n",
    "        if match:\n",
    "            processed_boxes.append(match.group(1))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    processed_boxes = []\n",
    "\n",
    "# NOTE: remove already processed boxes\n",
    "files = {\n",
    "    box_id: filenames\n",
    "    for box_id, filenames in files.items()\n",
    "    if box_id not in processed_boxes\n",
    "}\n",
    "\n",
    "for box_id, filenames in files.items():\n",
    "    print(f\"Processing box {box_id} with {len(filenames)} files\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dask_geopandas.read_parquet(\n",
    "    filenames, filesystem=fs, gather_spatial_partitions=False, columns=[\"transect_id\"]\n",
    ").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = []\n",
    "for p in ddf.partitions:\n",
    "    r.append(p.transect_id.unique().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "with fs.open(\n",
    "    \"az://shorelinemonitor-raw-series/release/2024-12-17/box_028.parquet\"\n",
    ") as f:\n",
    "    # with fs.open(\"az://shorelinemonitor-raw-series/release/2024-12-17/box_142.parquet\") as f:\n",
    "    df = gpd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_divisions(\n",
    "    index_array: np.ndarray | pd.Series, npartitions: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate division indices for repartitioning a Dask dataframe.\n",
    "\n",
    "    Args:\n",
    "        index_array (Union[np.ndarray, pd.Series]): Array of index values.\n",
    "        npartitions (int): Desired number of partitions.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Division indices for repartitioning.\n",
    "\n",
    "    Example:\n",
    "        >>> coastline_names = ddf[\"coastline_name\"].unique().compute()\n",
    "        >>> divisions = calculate_divisions(coastline_names, 20)\n",
    "    \"\"\"\n",
    "    if isinstance(index_array, pd.Series):\n",
    "        index_array = index_array.values\n",
    "\n",
    "    step = math.ceil(len(index_array) / npartitions)\n",
    "    divisions = np.concatenate([index_array[0:-1:step], [index_array[-1]]])\n",
    "    return divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transect_ids = ddf[\"transect_id\"].compute()\n",
    "divisions = calculate_divisions(transect_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def split_dataframe_by_unique_ids(df, max_rows):\n",
    "    \"\"\"\n",
    "    Split a sorted DataFrame into partitions, ensuring unique `transect_id` values per partition.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame, already sorted by `transect_quadkey` and `transect_id`.\n",
    "        max_rows (int): Approximate maximum number of rows per partition.\n",
    "\n",
    "    Returns:\n",
    "        list[pd.DataFrame]: A list of DataFrame partitions with unique `transect_id` values.\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    if total_rows == 0:\n",
    "        return []\n",
    "\n",
    "    # Estimate the number of partitions\n",
    "    num_partitions = int(np.ceil(total_rows / max_rows))\n",
    "    step_size = total_rows // num_partitions\n",
    "\n",
    "    # Identify initial split indices\n",
    "    split_indices = list(range(step_size, total_rows, step_size))\n",
    "\n",
    "    # Adjust indices to ensure unique `transect_id` per partition\n",
    "    adjusted_indices = []\n",
    "    for idx in split_indices:\n",
    "        while (\n",
    "            idx < len(df)\n",
    "            and df.loc[idx, \"transect_id\"] == df.loc[idx - 1, \"transect_id\"]\n",
    "        ):\n",
    "            idx += 1\n",
    "        if idx < len(df):  # Avoid adding out-of-bounds indices\n",
    "            adjusted_indices.append(idx)\n",
    "\n",
    "    # Create partitions\n",
    "    partitions = []\n",
    "    start_idx = 0\n",
    "    for idx in adjusted_indices:\n",
    "        partitions.append(df.iloc[start_idx:idx])\n",
    "        start_idx = idx\n",
    "\n",
    "    # Add the remaining rows as the last partition\n",
    "    if start_idx < len(df):\n",
    "        partitions.append(df.iloc[start_idx:])\n",
    "\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "with fs.open(\n",
    "    \"az://shorelinemonitor-raw-series/release/2024-12-17/box_028.parquet\"\n",
    ") as f:\n",
    "    # with fs.open(\"az://shorelinemonitor-raw-series/release/2024-12-17/box_142.parquet\") as f:\n",
    "    df = gpd.read_parquet(f)\n",
    "    print(df.shape)\n",
    "\n",
    "df = add_geo_columns(df, geo_columns=[\"quadkey\", \"bbox\"], quadkey_zoom_level=12)\n",
    "quadkeys = add_geo_columns(\n",
    "    gpd.GeoSeries.from_xy(df.transect_lon, df.transect_lat, crs=4326).to_frame(\n",
    "        \"geometry\"\n",
    "    ),\n",
    "    geo_columns=[\"quadkey\"],\n",
    ")\n",
    "df[\"transect_quadkey\"] = quadkeys.quadkey\n",
    "df = df.sort_values([\"transect_quadkey\", \"transect_id\", \"datetime\"]).reset_index(\n",
    "    drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = split_dataframe_by_unique_ids(df, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "for df_ in r:\n",
    "    ids.append(df_.transect_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_partition_uniqueness(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(r).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_ in r:\n",
    "    print(df_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "100 / (328707 / len(df)) / 2.83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coastpy.geo.size import estimate_memory_usage_per_row\n",
    "\n",
    "estimate_memory_usage_per_row(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1e6 rows per partition\n",
    "from coastpy.utils.size import size_to_bytes\n",
    "\n",
    "size_to_bytes(\"100MB\") / 319"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[0].isin(r[3]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_ids = set()\n",
    "for p in r:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_partition_uniqueness(r):\n",
    "    \"\"\"\n",
    "    Check if transect_id values are unique across partitions.\n",
    "    \"\"\"\n",
    "    # Initialize a set to track seen IDs and another for duplicates\n",
    "    seen_ids = set()\n",
    "    duplicates = set()\n",
    "\n",
    "    # Check uniqueness across partitions\n",
    "    for unique_ids in r:\n",
    "        for transect_id in unique_ids:\n",
    "            if transect_id in seen_ids:\n",
    "                duplicates.add(transect_id)\n",
    "            else:\n",
    "                seen_ids.add(transect_id)\n",
    "\n",
    "    # Determine if all transect_ids are unique across partitions\n",
    "    is_unique = len(duplicates) == 0\n",
    "\n",
    "    return is_unique, duplicates\n",
    "\n",
    "\n",
    "check_partition_uniqueness(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a set to track seen IDs\n",
    "seen_ids = set()\n",
    "duplicates = set()\n",
    "\n",
    "# Check uniqueness across partitions\n",
    "for unique_ids in r:\n",
    "    for transect_id in unique_ids:\n",
    "        if transect_id in seen_ids:\n",
    "            duplicates.add(transect_id)\n",
    "        else:\n",
    "            seen_ids.add(transect_id)\n",
    "\n",
    "# Report duplicates if any\n",
    "if duplicates:\n",
    "    print(\"Duplicate transect IDs found across partitions:\", duplicates)\n",
    "else:\n",
    "    print(\"All transect IDs are unique across partitions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in ddf.partitions:\n",
    "    p\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.compute(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coastal-full] *",
   "language": "python",
   "name": "conda-env-coastal-full-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
