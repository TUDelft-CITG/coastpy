{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3af02ead-aeaa-4329-b3cd-7340a6036b0a",
   "metadata": {},
   "source": [
    "# Hypsometry: how much of the first km coast is below 5 meters? \n",
    "\n",
    "This case study demonstrates the use of cloud-native workflows, combining data-proximate computing with cloud-optimized data exposed in a Spatio-Temporal Asset Collection to facilitate coastal analytics at a planetary scale. Specifically, we integrate the GCTS with DeltaDTM, a novel digital terrain model (Pronk et al., 2024), to determine the percentage of the world's first kilometer of coastal zone that is lower than 5 meters. For more information, please refer to Calkoen et al., Enabling Coastal Analytics at Planetary Scale (2024). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f397d6e-7ee5-440e-939e-25870b8ed64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from coastpy.utils.config import configure_instance\n",
    "\n",
    "instance_type = configure_instance()\n",
    "\n",
    "from odc.stac import configure_rio\n",
    "\n",
    "configure_rio(cloud_defaults=True)\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "import colorcet as cc\n",
    "import dask\n",
    "\n",
    "dask.config.set({\"dataframe.query-planning\": False})\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas\n",
    "import fsspec\n",
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pystac\n",
    "import pystac_client\n",
    "import rioxarray\n",
    "import shapely\n",
    "import xarray as xr\n",
    "from dotenv import load_dotenv\n",
    "from geopandas.array import GeometryDtype\n",
    "from ipyleaflet import Map, basemaps\n",
    "\n",
    "from coastpy.geo.quadtiles import make_mercantiles\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "sas_token = os.getenv(\"AZURE_STORAGE_SAS_TOKEN\")\n",
    "storage_account_name = os.getenv(\"AZURE_STORAGE_ACCOUNT\")\n",
    "storage_options = {\"account_name\": storage_account_name, \"sas_token\": sas_token}\n",
    "\n",
    "LOWER_THAN = (\n",
    "    5  # threshold in meter to compute \"xx of the 1-km coastal zone is <LOWER_THAN> m.\"\n",
    ")\n",
    "H3_LEVEL = 7  # h3 hexagonal-grid zoom-level to aggregate the results\n",
    "MERCANTILES_LEVEL = 4\n",
    "\n",
    "COMPUTE_GCTS_LANDWARD = False\n",
    "COMPUTE_GCTS_ELEVATION = False\n",
    "COMPUTE_H3_ELEVATION_STATISTICS = False\n",
    "COMPUTE_GLOBAL_LT_STATISTIC = False\n",
    "\n",
    "GCTS_LANDWARD_CONTAINER = \"az://public/coastal-analytics/gcts-2000m-landward.parquet\"\n",
    "# NOTE: before we stored the results here, keep for a while as ref\n",
    "GCTS_ELEVATION_CONTAINER = \"az://public/coastal-analytics/gcts-2000m-elevation.parquet\"\n",
    "# NOTE: Next iteration we will store it here and extract the profiles including the tr_name\n",
    "# GCTS_ELEVATION_CONTAINER = \"az://coastal-transect-repository/deltadtm-elevation.parquet\"\n",
    "H3_ELEVATION_CONTAINER = (\n",
    "    f\"az://public/coastal-analytics/h3-l{H3_LEVEL}-pct-lt-{LOWER_THAN}m.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5866580-7c5f-4c36-9b19-e36d42d8fa54",
   "metadata": {},
   "source": [
    "## Connect to the CoCliCo STAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb70455-73cd-4598-aa77-e690c1b7e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coastpy.io.utils import read_items_extent\n",
    "\n",
    "# Read the CoCliCo STAC catalog\n",
    "coclico_catalog = pystac.Catalog.from_file(\n",
    "    \"https://coclico.blob.core.windows.net/stac/v1/catalog.json\"\n",
    ")\n",
    "\n",
    "# Read the spatial footprint of the transect partitions.\n",
    "gcts_collection = coclico_catalog.get_child(\"gcts\")\n",
    "gcts_extents = read_items_extent(gcts_collection, columns=[\"geometry\", \"assets\"])\n",
    "\n",
    "# Read the spatial footprint of the DeltaDTM tiles.\n",
    "ddtm_collection = coclico_catalog.get_child(\"deltares-delta-dtm\")\n",
    "ddtm_extents = read_items_extent(ddtm_collection, columns=[\"geometry\", \"assets\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc0823-e4b8-41a2-bd38-f6fe34911230",
   "metadata": {},
   "source": [
    "## Make a compute cluster\n",
    "\n",
    "Depending on the instance type that is stored in `instance_type`, launch a Dask Gateway or LocalCluster, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45c78a2-d3bd-4832-8eff-92e2df371d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coastpy.utils.dask_utils import create_dask_client\n",
    "\n",
    "instance_type = configure_instance()\n",
    "client = create_dask_client(instance_type)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5218e2-ecb5-4755-a04a-6f1fe4757fed",
   "metadata": {},
   "source": [
    "## Part 1: Compute the landward part of the cross-shore transects\n",
    "\n",
    "The cross-shore coastal transects are 2km long, extending 1km in both land and see-ward direction. The statistics are computed over the 1km landward side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe928d-fc62-4b39-8d14-df82349d8534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if COMPUTE_GCTS_LANDWARD:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    def map_extract_landward_side(df):\n",
    "        def extract_landward_side(row):\n",
    "            p1 = shapely.Point(row.geometry.coords[0])\n",
    "            p2 = shapely.Point(row.lon, row.lat)\n",
    "            return shapely.LineString([p1, p2])\n",
    "\n",
    "        geoms = df.apply(extract_landward_side, axis=1)\n",
    "        return gpd.GeoDataFrame(df[\"tr_name\"], geometry=geoms, crs=4326)\n",
    "\n",
    "    gcts_collection = coclico_catalog.get_child(\"gcts\")\n",
    "    gcts_extents = read_items_extent(gcts_collection, columns=[\"geometry\", \"assets\"])\n",
    "    gcts_hrefs = gcts_extents.href.to_list()\n",
    "\n",
    "    # template GDF that matches what is retunred from map_extract_landward_side\n",
    "    META = gpd.GeoDataFrame(\n",
    "        {\n",
    "            \"tr_name\": gpd.GeoSeries([], dtype=str),\n",
    "            \"geometry\": gpd.GeoSeries([], dtype=GeometryDtype()),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    transects = dask_geopandas.read_parquet(\n",
    "        gcts_hrefs,\n",
    "        storage_options=storage_options,\n",
    "        columns=[\"tr_name\", \"geometry\", \"lon\", \"lat\"],\n",
    "    )\n",
    "\n",
    "    transects = transects.map_partitions(map_extract_landward_side, meta=META)\n",
    "    transects.to_parquet(GCTS_LANDWARD_CONTAINER, storage_options=storage_options)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time (H:M:S): {time.strftime('%H:%M:%S', time.gmtime(elapsed_time))}\")\n",
    "    client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aebee49-c528-41fb-b030-fa46005dfd80",
   "metadata": {},
   "source": [
    "## Part 2: Extract elevation data per transect\n",
    "\n",
    "1) First filter out the files that have already been processed\n",
    "2) Extract the elevation data per transect for each Deltares DeltaDTM tile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406239e-8663-43d8-9f28-4ec6d6aa0a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if COMPUTE_GCTS_ELEVATION:\n",
    "\n",
    "    from dask.distributed import TimeoutError\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    def to_out_href(href, out_prefix):\n",
    "        return out_prefix + \"/\" + href.split(\"_\")[-1].split(\".\")[0] + \".parquet\"\n",
    "\n",
    "    @dask.delayed\n",
    "    def extract_by_geometry(href, transects, storage_options):\n",
    "        \"\"\"Extract elevation raster data by transect LineString geometry in columnar format.\"\"\"\n",
    "\n",
    "        da = rioxarray.open_rasterio(href, chunks={}, lock=False).squeeze().drop(\"band\")\n",
    "\n",
    "        bbox = da.rio.bounds()\n",
    "        bbox = gpd.GeoDataFrame(geometry=[shapely.box(*bbox)], crs=4326)\n",
    "\n",
    "        transects = gpd.overlay(transects, bbox)\n",
    "\n",
    "        if transects.empty:\n",
    "            return\n",
    "\n",
    "        da = da.where(da != da.rio.nodata, np.nan)\n",
    "        da = da.rio.write_nodata(np.nan)\n",
    "\n",
    "        # TODO: ensure that tr_name is tracked so that we can use the elevation data later at a transect level\n",
    "        clipped = da.rio.clip(transects.geometry.to_list()).rename(\"band_data\")\n",
    "\n",
    "        df = (\n",
    "            clipped.drop([\"spatial_ref\"])\n",
    "            .to_dataframe()\n",
    "            .dropna()\n",
    "            .reset_index()\n",
    "            .rename(columns={\"x\": \"lng\", \"y\": \"lat\"})\n",
    "            .rename(columns={\"band_data\": \"z\"})[[\"lng\", \"lat\", \"z\"]]\n",
    "        )\n",
    "\n",
    "        out_href = to_out_href(href, GCTS_ELEVATION_CONTAINER)\n",
    "\n",
    "        with fsspec.open(out_href, \"wb\", **storage_options) as f:\n",
    "            df.to_parquet(f)\n",
    "\n",
    "    ddtm_collection = coclico_catalog.get_child(\"deltares-delta-dtm\")\n",
    "    ddtm_extents = read_items_extent(ddtm_collection, columns=[\"geometry\", \"assets\"])\n",
    "\n",
    "    tiles = make_mercantiles(MERCANTILES_LEVEL)\n",
    "\n",
    "    ddtm_extents = (\n",
    "        gpd.sjoin(ddtm_extents, tiles)\n",
    "        .drop(columns=\"index_right\")\n",
    "        .sample(frac=1)\n",
    "        .drop_duplicates(\"href\", keep=\"first\")\n",
    "        .sort_index()\n",
    "    )\n",
    "    ddtm_extents[\"out_href\"] = ddtm_extents.href.map(\n",
    "        lambda href: to_out_href(href, GCTS_ELEVATION_CONTAINER)\n",
    "    )\n",
    "\n",
    "    fs = fsspec.filesystem(\"az\", **storage_options)\n",
    "    processed_files = fs.glob(f\"{GCTS_ELEVATION_CONTAINER}/*.parquet\")\n",
    "    processed_files = [f\"az://{f}\" for f in processed_files]\n",
    "\n",
    "    ddtm_extents = ddtm_extents.loc[~ddtm_extents[\"out_href\"].isin(processed_files)]\n",
    "    print(f\"Number of DeltaDTM tiles to process: {len(ddtm_extents)}\")\n",
    "\n",
    "    for name, gr in ddtm_extents.groupby(\"quadkey\"):\n",
    "        print(\n",
    "            f\"Start processing quadkey {name}, with total bounds: {gr.total_bounds}, that in total consist of {len(gr)} DeltaDTM tiles.\"\n",
    "        )\n",
    "\n",
    "        roi = gpd.GeoDataFrame(geometry=[shapely.box(*gr.total_bounds)], crs=4326)\n",
    "        ddtm_hrefs = gr.href.to_list()\n",
    "\n",
    "        client.restart(wait_for_workers=False)\n",
    "\n",
    "        try:\n",
    "            transects = (\n",
    "                dask_geopandas.read_parquet(\n",
    "                    GCTS_LANDWARD_CONTAINER, storage_options=storage_options\n",
    "                )\n",
    "                .sjoin(roi)\n",
    "                .drop(columns=[\"index_right\"])\n",
    "                .compute(timeout=\"5m\")\n",
    "            )\n",
    "        except TimeoutError:\n",
    "            print(\"Loading transects timed out after 5 minutes.\")\n",
    "            continue\n",
    "\n",
    "        transects_scattered = client.scatter(transects, broadcast=True)\n",
    "\n",
    "        tasks = []\n",
    "        for href in ddtm_hrefs:\n",
    "            href = f\"https://coclico.blob.core.windows.net/{href.strip('az://')}?{sas_token}\"\n",
    "            tasks.append(\n",
    "                extract_by_geometry(href, transects_scattered, storage_options)\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            dask.compute(\n",
    "                *tasks, timeout=\"8m\"\n",
    "            )  # 8 minutes timeout for the whole operation\n",
    "        except TimeoutError:\n",
    "            print(\"The operation timed out after 8 minutes.\")\n",
    "            continue\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time (H:M:S): {time.strftime('%H:%M:%S', time.gmtime(elapsed_time))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b70a98d-51c2-4bb8-a80b-6b7bb6ec938b",
   "metadata": {},
   "source": [
    "## Part 3: Computing statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afdf363-28ef-4015-bd61-e49f8a33bbb1",
   "metadata": {},
   "source": [
    "#### Install required dependencies on all workers on the cluster\n",
    "\n",
    "Please note that this step is only required for Planetary Computer as H3-Pandas is not included in their container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc9e084-e7d1-4290-a967-0361dfae68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_H3_ELEVATION_STATISTICS:\n",
    "    import os\n",
    "\n",
    "    install_cmd = \"mamba install h3pandas -y\"\n",
    "\n",
    "    def worker_setup(dask_worker):\n",
    "        import os\n",
    "\n",
    "        os.system(install_cmd)\n",
    "\n",
    "    if instance_type.name == \"MSPC\":\n",
    "        client.register_worker_callbacks(worker_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a86abf4-b398-439b-ae41-157cf301dbbe",
   "metadata": {},
   "source": [
    "### Part 3.1: computing statistics by binning into H3 hexagons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6c824-4e7f-4e97-823b-32b7eb09b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_H3_ELEVATION_STATISTICS:\n",
    "    import h3pandas\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    gr_key = f\"h3_{H3_LEVEL:02d}\"\n",
    "\n",
    "    DTYPES = {\n",
    "        f\"{gr_key}\": \"object\",\n",
    "        f\"pct_lt_{LOWER_THAN}m\": \"int32\",\n",
    "        \"n_obs\": \"int32\",\n",
    "        \"geometry\": GeometryDtype(),\n",
    "        \"lon\": \"float32\",\n",
    "        \"lat\": \"float32\",\n",
    "    }\n",
    "\n",
    "    # Create META using DTYPES\n",
    "    META = pd.DataFrame({col: pd.Series(dtype=dt) for col, dt in DTYPES.items()})\n",
    "\n",
    "    def lower_than(group, threshold):\n",
    "        count = (group[\"z\"] < threshold).sum()\n",
    "        total_count = group[\"z\"].count()\n",
    "        percentage = (count / total_count) * 100\n",
    "        return pd.Series(\n",
    "            [percentage, total_count], index=[f\"pct_lt_{threshold}m\", \"n_obs\"]\n",
    "        )\n",
    "\n",
    "    def add_h3(df, level, threshold, meta):\n",
    "        import h3pandas\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        if df.empty:\n",
    "            # Ensure the empty DataFrame matches the META structure\n",
    "            return pd.DataFrame([], columns=meta.columns).astype(meta.dtypes)\n",
    "\n",
    "        df = df.h3.geo_to_h3(level)\n",
    "\n",
    "        # Convert level to string with leading zeros if necessary\n",
    "        level_str = f\"{level:02d}\"  # Ensures two digits\n",
    "        h3_group_key = f\"h3_{level_str}\"\n",
    "        result = (\n",
    "            df.drop(columns=[\"lng\", \"lat\"])\n",
    "            .groupby(h3_group_key)\n",
    "            .apply(lambda gr: lower_than(gr, threshold))\n",
    "            .reset_index()\n",
    "        )\n",
    "        result = result.set_index(h3_group_key).h3.h3_to_geo_boundary().reset_index()\n",
    "\n",
    "        points = result.set_index(\"h3_07\").h3.h3_to_geo().geometry\n",
    "        result[\"lon\"] = points.x.values\n",
    "        result[\"lat\"] = points.y.values\n",
    "\n",
    "        # Ensure the result matches the META structure and data types\n",
    "        result = result.astype(meta.dtypes.to_dict())\n",
    "\n",
    "        return result\n",
    "\n",
    "    ddf = dd.read_parquet(\n",
    "        GCTS_ELEVATION_CONTAINER + \"/*.parquet\",\n",
    "        storage_options=storage_options,\n",
    "    )\n",
    "\n",
    "    ddf = ddf.map_partitions(\n",
    "        lambda df: add_h3(df, level=H3_LEVEL, threshold=LOWER_THAN, meta=META),\n",
    "        meta=META,\n",
    "    )\n",
    "    df = ddf.compute().reset_index(drop=True)\n",
    "\n",
    "    with fsspec.open(H3_ELEVATION_CONTAINER, mode=\"wb\", **storage_options) as f:\n",
    "        df.to_parquet(f)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time (H:M:S): {time.strftime('%H:%M:%S', time.gmtime(elapsed_time))}\")\n",
    "\n",
    "    client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e381ae00-a98e-482b-bd32-2f735102c3c0",
   "metadata": {},
   "source": [
    "### Part 3.2: Compute global statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9394e490-3b68-4b09-9d1f-2b0e5d040c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "if COMPUTE_GLOBAL_LT_STATISTIC:\n",
    "\n",
    "    ddf = dd.read_parquet(\n",
    "        GCTS_ELEVATION_CONTAINER + \"/*.parquet\", storage_options=storage_options\n",
    "    )\n",
    "\n",
    "    pct_lt_5m = ((ddf[\"z\"] < LOWER_THAN).mean() * 100).compute()\n",
    "    print(\n",
    "        f\"We find that {round(pct_lt_5m)}% is lower than {LOWER_THAN}m in the 1km coastal zone.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f9da7-1783-4778-abcf-978382f1b603",
   "metadata": {},
   "source": [
    "## Part 4: Visualization\n",
    "\n",
    "For the visualization we mostly use Holoviews with a Bokeh backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb723c32-7958-4dd4-a65e-c4ac12cf8fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import colorcet as cc\n",
    "import geopandas as gpd\n",
    "import geoviews as gv\n",
    "import h3pandas\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import panel as pn\n",
    "from bokeh.io import export_svg\n",
    "from cartopy import crs as ccrs\n",
    "from holoviews import opts\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c256232-4f06-4cd4-a97f-c568ce5cfb38",
   "metadata": {},
   "source": [
    "### Part 4.1: Read the elevation statistics from cloud object store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c0cfc6-5283-461f-afe4-9d5f696e2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fsspec.open(H3_ELEVATION_CONTAINER, mode=\"rb\", **storage_options) as f:\n",
    "    h3_elevation = gpd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7050fba1-4b27-4ea5-890d-c795be7de5c1",
   "metadata": {},
   "source": [
    "### Part 4.2: Aggregate data on lower H3 grid to avoid overplotting\n",
    "\n",
    "Additionaly we filter out the H3 hexagon's that cross antimerdian boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e41ef-1457-4b45-b06d-c6020dcb3020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosses_antimeridian(geom):\n",
    "    \"\"\"\n",
    "    Checks if a geometry crosses the anti-meridian.\n",
    "    \"\"\"\n",
    "    longitudes = [point[0] for point in geom.exterior.coords]\n",
    "    return any(\n",
    "        abs(longitudes[i] - longitudes[i - 1]) > 180 for i in range(1, len(longitudes))\n",
    "    )\n",
    "\n",
    "\n",
    "def to_h3_agg(df, zoom):\n",
    "    h3_agg = (\n",
    "        df[[\"h3_07\", \"pct_lt_5m\"]]\n",
    "        .set_index(\"h3_07\")\n",
    "        .h3.h3_to_parent(zoom)\n",
    "        .set_index(f\"h3_0{zoom}\")\n",
    "        .groupby(f\"h3_0{zoom}\")\n",
    "        .mean()\n",
    "        .h3.h3_to_geo_boundary()\n",
    "    )\n",
    "\n",
    "    h3_agg[\"crosses_antimeridian\"] = (\n",
    "        h3_agg[\"geometry\"].astype(object).apply(crosses_antimeridian)\n",
    "    )\n",
    "    h3_agg = h3_agg.loc[h3_agg[\"crosses_antimeridian\"] == False]\n",
    "    return h3_agg\n",
    "\n",
    "\n",
    "zoom = 3\n",
    "h3_agg = to_h3_agg(h3_elevation, zoom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d118e98-f963-43f1-a7da-61acc2ca9d26",
   "metadata": {},
   "source": [
    "## Part 4.3: Plot global distribution of coastal land that lower than 5 meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a60b5-0bec-4ee4-b085-69ee51c2409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_lt_5m_plot = h3_agg[[\"pct_lt_5m\", \"geometry\"]].hvplot(\n",
    "    geo=True,\n",
    "    cmap=cc.CET_L20[::-1],\n",
    "    color=\"pct_lt_5m\",\n",
    "    size=8,\n",
    "    projection=ccrs.Robinson(),\n",
    "    features=[\"land\"],\n",
    "    alpha=0.75,\n",
    "    colorbar=False,\n",
    "    width=539,  # Env. modeling fig instructions\n",
    "    global_extent=True,\n",
    ")\n",
    "\n",
    "global_lt_5m_plot.opts(\n",
    "    hv.opts.Polygons(\n",
    "        # colorbar_position=\"left\",\n",
    "        line_color=None,\n",
    "        colorbar_opts={\n",
    "            \"title\": None,\n",
    "            # \"title\": \"Lower than 5 m (%)\",\n",
    "            # \"orientation\": \"vertical\",\n",
    "            # \"width\": 15,\n",
    "            # \"height\": 400,\n",
    "            \"scale_alpha\": 1,\n",
    "        },\n",
    "        toolbar=\"disable\",\n",
    "    )\n",
    ")\n",
    "\n",
    "grid = gv.feature.grid(\n",
    "    projection=ccrs.Robinson(), title=\"\", fill_color=\"none\", color=\"gray\"\n",
    ")\n",
    "\n",
    "global_lt_5m_plot = global_lt_5m_plot * grid\n",
    "global_lt_5m_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e598c7-9cee-4e4b-bca6-6bc746e7d6c5",
   "metadata": {},
   "source": [
    "#### Export as SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c93d743-08d4-4c47-840c-1b0d91fe0cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "H3_GLOBAL_LT_5M_FP = (\n",
    "    f\"az://figures/enabling-coastal-analytics/h3_0{zoom}-global-lt-5m.svg\"\n",
    ")\n",
    "\n",
    "renderer = hv.renderer(\"bokeh\")\n",
    "bokeh_figure = renderer.get_plot(global_lt_5m_plot).state\n",
    "bokeh_figure.output_backend = \"svg\"\n",
    "bokeh_figure.background_fill_color = None\n",
    "bokeh_figure.border_fill_color = None\n",
    "\n",
    "# outpath = (pathlib.Path(\"~\") / H3_GLOBAL_LT_5M_FP.strip(\"az://\")).expanduser()\n",
    "# outpath.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# with outpath.open(mode=\"w\") as fp:\n",
    "#     export_svg(bokeh_figure, filename=fp.name)\n",
    "\n",
    "#     # Write to cloud storage\n",
    "#     with fsspec.open(H3_GLOBAL_LT_5M_FP, mode=\"wb\", **storage_options) as cloud_file:\n",
    "#         with open(fp.name, mode=\"rb\") as fp_read:\n",
    "#             cloud_file.write(fp_read.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2982971-1f53-459b-9f2f-69325f39fd08",
   "metadata": {},
   "source": [
    "### Part 4.4: Hotspot analysis using spatial correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5221221-e41f-4a0f-82e0-4f0a0d1c00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esda import Moran_Local\n",
    "from libpysal import weights\n",
    "\n",
    "\n",
    "def add_lisa(gdf):\n",
    "    W = weights.Queen.from_dataframe(gdf)\n",
    "    W.transform = \"r\"\n",
    "\n",
    "    # LISA analysis\n",
    "    lisa = Moran_Local(gdf[\"pct_lt_5m\"].values, W)\n",
    "\n",
    "    # Identifying significant hotspots\n",
    "    gdf[\"lisa_q\"] = (\n",
    "        lisa.q\n",
    "    )  # The quadrant they belong to; 1 (HH), 2 (LH), 3 (LL), 4 (HL)\n",
    "    gdf[\"p_val\"] = lisa.p_sim  # P-values for assessing significance\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def format_hotspot_df(df):\n",
    "    def compute_area(df, utm_epsg):\n",
    "        df = df.to_crs(utm_epsg)\n",
    "        return df.geometry.area\n",
    "\n",
    "    with fsspec.open(\n",
    "        \"https://coclico.blob.core.windows.net/public/utm_grid.parquet\"\n",
    "    ) as f:\n",
    "        utm_grid = gpd.read_parquet(f)\n",
    "\n",
    "    df = df.dissolve().explode().reset_index(drop=True)\n",
    "\n",
    "    df[\"utm_epsg\"] = gpd.sjoin(\n",
    "        df.geometry.representative_point().to_frame(\"geometry\"), utm_grid\n",
    "    )[\"epsg\"]\n",
    "\n",
    "    area = (\n",
    "        df.groupby(\"utm_epsg\")\n",
    "        .apply(lambda gr: compute_area(gr, gr.name))\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"area\"})\n",
    "        .sort_values(\"level_1\")\n",
    "        .reset_index(drop=True)[[\"area\"]]\n",
    "    )\n",
    "\n",
    "    df = df.join(area).sort_values(\"area\", ascending=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "h3_agg2 = to_h3_agg(h3_elevation, 5)\n",
    "h3_agg2 = add_lisa(h3_agg2)\n",
    "\n",
    "hotspots = h3_agg2[(h3_agg2[\"lisa_q\"] == 1) & (h3_agg2[\"p_val\"] < 0.05)]\n",
    "hotspots = format_hotspot_df(hotspots)\n",
    "hotspots = hotspots.iloc[: int(len(hotspots) * 0.025)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e071aa-2e6a-4b54-a31d-bf23651da83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_plot = global_lt_5m_plot * hotspots.assign(\n",
    "    geometry=hotspots.representative_point()\n",
    ").hvplot.points(\n",
    "    geo=True,\n",
    "    size=\"area\",\n",
    "    color=\"red\",\n",
    "    scale=0.00003,\n",
    "    alpha=0.6,\n",
    "    projection=ccrs.Robinson(),\n",
    ")\n",
    "\n",
    "hotspots_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d538db5-46ba-46b8-9519-1fb7ad453fad",
   "metadata": {},
   "source": [
    "#### Export hotspots as SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf76eb4a-226d-4742-afdb-bab15b223dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "H3_GLOBAL_LT_5M_HOTSPOTS_FP = (\n",
    "    f\"az://figures/enabling-coastal-analytics/h3_0{zoom}-global-lt-5m-hotspots.svg\"\n",
    ")\n",
    "\n",
    "renderer = hv.renderer(\"bokeh\")\n",
    "bokeh_figure = renderer.get_plot(hotspots_plot).state\n",
    "bokeh_figure.output_backend = \"svg\"\n",
    "bokeh_figure.background_fill_color = None\n",
    "bokeh_figure.border_fill_color = None\n",
    "\n",
    "# outpath = (pathlib.Path(\"~\") / H3_GLOBAL_LT_5M_HOTSPOTS_FP.strip(\"az://\")).expanduser()\n",
    "# outpath.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# with outpath.open(mode=\"w\") as fp:\n",
    "#     export_svg(bokeh_figure, filename=fp.name)\n",
    "\n",
    "#     # Write to cloud storage\n",
    "#     with fsspec.open(H3_GLOBAL_LT_5M_FP, mode=\"wb\", **storage_options) as cloud_file:\n",
    "#         with open(fp.name, mode=\"rb\") as fp_read:\n",
    "#             cloud_file.write(fp_read.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a56ff-9af4-42f4-a400-43335e11d08b",
   "metadata": {},
   "source": [
    "### Part 4:5: Violin plot of the global coastal elevation in the first km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7f055d-c313-474b-a024-06a4da455bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fsspec.open(H3_ELEVATION_CONTAINER, mode=\"rb\", **storage_options) as f:\n",
    "    h3_elevation = gpd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7acfd-30ab-4714-aad1-ac257db11357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def width_in_pixels(mm, dpi):\n",
    "    return int((mm / 25.4) * dpi)\n",
    "\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "\n",
    "# Assuming h3_elevation is a pandas DataFrame with the column 'pct_lt_5m' containing the data\n",
    "violin_plot = hv.Violin(h3_elevation[[\"pct_lt_5m\"]], vdims=\"pct_lt_5m\")\n",
    "\n",
    "# Configure the plot options\n",
    "violin_plot = violin_plot.opts(\n",
    "    opts.Violin(\n",
    "        inner=\"quartiles\",\n",
    "        cmap=cc.CET_L20,\n",
    "        # yticks=[(i, f\"{i}\") for i in np.arange(0, 101, 20)],\n",
    "        # xticks=None,\n",
    "        ylabel=\"\",\n",
    "        # fontsize={\"ticks\": 9},\n",
    "        width=width_in_pixels(mm=140 * (1 / 3), dpi=97),\n",
    "        cut=0,  # clips violin to (0,100)\n",
    "        border=0,\n",
    "        padding=0,\n",
    "        show_frame=False,\n",
    "        yaxis=\"right\",\n",
    "    )\n",
    ")\n",
    "\n",
    "violin_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d204790e-6b3c-46ad-b111-117426feb17c",
   "metadata": {},
   "source": [
    "#### Export to svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ea781-a8f6-4357-9dd4-153d4cf58b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "H3_GLOBAL_DISTRIBUTION_LT_5M = (\n",
    "    f\"az://figures/enabling-coastal-analytics/h3_0{zoom}-global-distribution-lt-5m.svg\"\n",
    ")\n",
    "\n",
    "outpath = (pathlib.Path(\"~\") / H3_GLOBAL_DISTRIBUTION_LT_5M.strip(\"az://\")).expanduser()\n",
    "outpath.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "renderer = hv.renderer(\"bokeh\")\n",
    "bokeh_figure = renderer.get_plot(violin_plot).state\n",
    "bokeh_figure.output_backend = \"svg\"\n",
    "# bokeh_figure.background_fill_color = None\n",
    "# bokeh_figure.border_fill_color = None\n",
    "# bokeh_figure.outline_line_color = None\n",
    "# bokeh_figure.outline_line_width = 0\n",
    "\n",
    "\n",
    "# with outpath.open(mode=\"w\") as fp:\n",
    "#     export_svg(bokeh_figure, filename=fp.name)\n",
    "\n",
    "#     # Write to cloud storage\n",
    "#     with fsspec.open(H3_GLOBAL_LT_5M_FP, mode=\"wb\", **storage_options) as cloud_file:\n",
    "#         with open(fp.name, mode=\"rb\") as fp_read:\n",
    "#             cloud_file.write(fp_read.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323527e-855a-4477-b325-947f2990d7fc",
   "metadata": {},
   "source": [
    "## Part 5: Figure with example of high-resolution data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8365bd-9082-428e-92f2-299a6e68ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import duckdb\n",
    "import geoviews.tile_sources as gvts\n",
    "import holoviews as hv\n",
    "import hvplot.pandas  # Ensures hvplot is available\n",
    "from bokeh.io import export_png\n",
    "from holoviews import opts\n",
    "from holoviews.operation.datashader import dynspread\n",
    "from ipyleaflet import Map, basemaps\n",
    "from shapely import wkb\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88130da-10c3-4e46-b221-d2797376c3a6",
   "metadata": {},
   "source": [
    "### Part 5.1 Select a region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef592cde-6419-459a-9883-2dc9bf7dbd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, basemaps\n",
    "\n",
    "m = Map(basemap=basemaps.Esri.WorldImagery, scroll_wheel_zoom=True)\n",
    "m.center = 15.825, -95.95\n",
    "m.zoom = 14\n",
    "m.layout.height = \"725px\"\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4a8f9-8d79-4d04-a194-530029e80e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = shapely.geometry.box(m.west, m.south, m.east, m.north)\n",
    "roi = gpd.GeoDataFrame(geometry=[roi], crs=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b91a5-7087-4799-9dbc-667b6f86051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCTS_CONTAINER = gcts_collection.extra_fields[\"base_url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e1f74-ab3e-4106-8ce4-9d0bb9fac525",
   "metadata": {},
   "source": [
    "### Part 5.2 Extract the data using DuckDB\n",
    "\n",
    "We do not have STACs for all data that was produced in this experiment, so we just use DuckDB to efficiently filter the container by predicate pushdown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa88e9-7345-4289-8304-22591340c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the bounding box coordinates from the region of interest (ROI)\n",
    "minx, miny, maxx, maxy = roi.total_bounds\n",
    "\n",
    "# Connect to an in-memory DuckDB instance and load necessary extensions\n",
    "con = duckdb.connect(database=\":memory:\", read_only=False)\n",
    "con.execute(\"INSTALL azure;\")\n",
    "con.execute(\"LOAD azure;\")\n",
    "con.execute(\"INSTALL spatial;\")\n",
    "con.execute(\"LOAD spatial;\")\n",
    "\n",
    "# Create a secret for Azure connection\n",
    "try:\n",
    "    con.execute(\n",
    "        f\"\"\"\n",
    "        CREATE SECRET azure_secret (\n",
    "        TYPE AZURE,\n",
    "        CONNECTION_STRING '{os.getenv('CLIENT_AZURE_STORAGE_CONNECTION_STRING')}');\n",
    "        \"\"\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"The secret is likely already available. Exception message:\\n\\n{e}\")\n",
    "\n",
    "# Define the query to read transect data within the bounding box\n",
    "transect_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_parquet('{GCTS_CONTAINER + \"/*.parquet\"}')\n",
    "    WHERE\n",
    "        bbox.xmin <= {maxx} AND\n",
    "        bbox.ymin <= {maxy} AND\n",
    "        bbox.xmax >= {minx} AND\n",
    "        bbox.ymax >= {miny};\n",
    "\"\"\"\n",
    "\n",
    "# Execute the transect query and fetch the result as a DataFrame\n",
    "transect_df = con.execute(transect_query).fetchdf()\n",
    "\n",
    "# Convert ROI geometry to WKT format\n",
    "roi_wkt = roi.geometry.to_wkt().iloc[0]\n",
    "\n",
    "# Create or replace the transects table and fetch the intersecting transects\n",
    "con.execute(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE transects AS \n",
    "    SELECT * FROM read_parquet('{GCTS_LANDWARD_CONTAINER + \"/*.parquet\"}');\n",
    "    \"\"\"\n",
    ")\n",
    "landward_transects_df = con.execute(\n",
    "    f\"\"\"\n",
    "    SELECT * \n",
    "    FROM transects \n",
    "    WHERE ST_Intersects(ST_GeomFromWKB(transects.geometry), ST_GeomFromText('{roi_wkt}'));\n",
    "    \"\"\"\n",
    ").fetchdf()\n",
    "\n",
    "# Define the query to read elevation data within the bounding box\n",
    "elevation_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_parquet('{GCTS_ELEVATION_CONTAINER + \"/*.parquet\"}')\n",
    "    WHERE\n",
    "        lng <= {maxx} AND\n",
    "        lat <= {maxy} AND\n",
    "        lng >= {minx} AND\n",
    "        lat >= {miny};\n",
    "\"\"\"\n",
    "\n",
    "# Execute the elevation query and fetch the result as a DataFrame\n",
    "elevation_df = con.execute(elevation_query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed2e77e-d434-4c39-b19c-d59a51b95d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transect_gdf = gpd.GeoDataFrame(\n",
    "    transect_df,\n",
    "    geometry=transect_df.geometry.apply(lambda x: wkb.loads(bytes(x))),\n",
    "    crs=4326,\n",
    ")\n",
    "\n",
    "landward_transects_gdf = gpd.GeoDataFrame(\n",
    "    landward_transects_df,\n",
    "    geometry=landward_transects_df.geometry.apply(lambda x: wkb.loads(bytes(x))),\n",
    "    crs=4326,\n",
    ")\n",
    "\n",
    "elevation_gdf = gpd.GeoDataFrame(\n",
    "    elevation_df,\n",
    "    geometry=gpd.points_from_xy(elevation_df.lng, elevation_df.lat, crs=4326),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619301b-ca78-4249-9df3-825ba76131db",
   "metadata": {},
   "source": [
    "#### Plot the elevation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd0294-012b-4978-8672-2a8e58b256c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_plot = elevation_gdf.hvplot(\n",
    "    geo=True, datashade=True, color=\"z\", cmap=cc.CET_L20, colorbar=True, width=800\n",
    ")\n",
    "dynspread(elevation_plot, threshold=0.5, max_px=200) * gvts.EsriImagery()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26beb3a-dea3-474f-a7fa-c0f7f6e97768",
   "metadata": {},
   "source": [
    "### Part 5.2: Adjust the area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ffc007-25fe-457c-a2f7-35897aca0990",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_m = Map(basemap=basemaps.Esri.WorldImagery, scroll_wheel_zoom=True)\n",
    "inner_m.center = 15.827, -95.96\n",
    "\n",
    "inner_m.zoom = 15\n",
    "inner_m.layout.height = \"725px\"\n",
    "inner_m.layout.width = \"725px\"\n",
    "inner_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e4357f-cd86-4efe-97ac-58b77de0b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_roi = shapely.geometry.box(\n",
    "    inner_m.west, inner_m.south, inner_m.east, inner_m.north\n",
    ")\n",
    "inner_roi_gdf = gpd.GeoDataFrame(geometry=[inner_roi], crs=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5cb715-5912-4f6b-961a-1b05bffb0ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transect_gdf_inner = transect_gdf.iloc[\n",
    "    gpd.sjoin(\n",
    "        gpd.GeoSeries.from_xy(transect_gdf.lon, transect_gdf.lat, crs=4326).to_frame(\n",
    "            \"geometry\"\n",
    "        ),\n",
    "        inner_roi_gdf,\n",
    "    ).index.to_list()\n",
    "]\n",
    "\n",
    "landward_transects_gdf_inner = gpd.sjoin(\n",
    "    landward_transects_gdf, transect_gdf_inner[[\"geometry\"]]\n",
    ").drop(columns=[\"index_right\"])\n",
    "\n",
    "# Create a buffer so that we only show elevation data at transects\n",
    "landward_transect_buffer = landward_transects_gdf_inner.to_crs(\n",
    "    landward_transects_gdf_inner.estimate_utm_crs()\n",
    ").buffer(50)\n",
    "\n",
    "elevation_gdf_inner = (\n",
    "    gpd.sjoin(\n",
    "        elevation_gdf.reset_index(),\n",
    "        landward_transect_buffer.to_crs(4326).to_frame(\"geometry\"),\n",
    "    )\n",
    "    .drop_duplicates(\"index\")\n",
    "    .drop(columns=[\"index\", \"index_right\"])\n",
    ")\n",
    "\n",
    "# transect_gdf_inner_bounds = gpd.GeoDataFrame(\n",
    "#     geometry=[shapely.geometry.box(*transect_gdf_inner.total_bounds)], crs=4326\n",
    "# )\n",
    "\n",
    "# elevation_gdf_inner = gpd.sjoin(elevation_gdf, transect_gdf_inner_bounds).drop(\n",
    "#     columns=[\"index_right\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27c54a1-6bd4-4835-b052-b398029a9e5f",
   "metadata": {},
   "source": [
    "### Part 5.3 Create the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceec199-ef16-4fe7-a3db-bebc9f1cfd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_plot = gvts.EsriImagery() * (\n",
    "    transect_gdf_inner[[\"geometry\"]].hvplot(\n",
    "        geo=True,\n",
    "        width=1200,\n",
    "        label=\"GCTS - seaward\",\n",
    "        xlabel=\"Longitude\",\n",
    "        ylabel=\"Latitude\",\n",
    "        line_width=4,\n",
    "        fontscale=3,\n",
    "    )\n",
    "    * elevation_gdf_inner.hvplot(\n",
    "        geo=True,\n",
    "        cmap=cc.CET_L20,\n",
    "        color=\"z\",\n",
    "        colorbar=True,\n",
    "        clabel=\"DeltaDTM elevation (m)\",\n",
    "        fontscale=3,\n",
    "        size=200,\n",
    "    )\n",
    "    * landward_transects_gdf_inner.hvplot(\n",
    "        geo=True,\n",
    "        color=\"red\",\n",
    "        alpha=1,\n",
    "        label=\"GCTS - landward\",\n",
    "        fontscale=3,\n",
    "        line_width=4,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Apply opts to customize legends and colorbar\n",
    "hr_plot.opts(\n",
    "    hv.opts.Overlay(\n",
    "        legend_position=\"bottom_right\",  # Position the legend at the bottom right\n",
    "        toolbar=\"disable\",  # Toolbar position can be adjusted here if needed\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the final plot\n",
    "pn.Column(hr_plot).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a324b3-ea47-488b-88ac-24466e02be5e",
   "metadata": {},
   "source": [
    "#### Export figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ccdbb-d71a-44f3-84c8-83606ea6849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "H3_HR_LT_5M = (\n",
    "    f\"az://figures/enabling-coastal-analytics/hr-data-extract-barra-de-la-cruz.png\"\n",
    ")\n",
    "\n",
    "outpath = (pathlib.Path(\"~\") / H3_HR_LT_5M.strip(\"az://\")).expanduser()\n",
    "outpath.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "renderer = hv.renderer(\"bokeh\")\n",
    "bokeh_figure = renderer.get_plot(hr_plot).state\n",
    "# bokeh_figure.output_backend = \"png\"\n",
    "bokeh_figure.background_fill_color = None\n",
    "bokeh_figure.border_fill_color = None\n",
    "\n",
    "\n",
    "# with outpath.open(mode=\"w\") as fp:\n",
    "#     export_png(bokeh_figure, filename=fp.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff883e74-2cf6-4a31-817b-d66de4be6dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coastal]",
   "language": "python",
   "name": "conda-env-coastal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
