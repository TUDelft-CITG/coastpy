{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Hypsometry: how much of the first km coast is below 5 meters? \n",
    "\n",
    "This case study demonstrates how we can globally analyze elevation data (rasters) at millions of coastal stations (vector geometries). More specifically, we combine the [Global Coastal Transect System (GCTS)](https://radiantearth.github.io/stac-browser/#/external/coclico.blob.core.windows.net/stac/v1/gcts/collection.json) (vector; LineString geometries) with [DeltaDTM](https://radiantearth.github.io/stac-browser/#/external/coclico.blob.core.windows.net/stac/v1/deltares-delta-dtm/collection.json) (raster, ~30m), a novel digital terrain model [Pronk et al., 2024](https://www.nature.com/articles/s41597-024-03091-9), to determine the percentage of the world's first kilometer of coast that is lower than 5 meters. Analyzing at 100 m alongshore resolution, we find that 33% of first km of coast is lower than 5 meters. For more information and results please see [Calkoen et al., Enabling Coastal Analytics at Planetary Scale (2025)](https://www.sciencedirect.com/science/article/pii/S1364815224003189). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from odc.stac import configure_rio\n",
    "\n",
    "configure_rio(cloud_defaults=True)\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "import colorcet as cc\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas\n",
    "import fsspec\n",
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pystac\n",
    "import pystac_client\n",
    "import rioxarray\n",
    "import shapely\n",
    "import xarray as xr\n",
    "from dotenv import load_dotenv\n",
    "from geopandas.array import GeometryDtype\n",
    "from ipyleaflet import Map, basemaps\n",
    "\n",
    "from coastpy.geo.quadtiles import make_mercantiles\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "sas_token = os.getenv(\"AZURE_STORAGE_SAS_TOKEN\")\n",
    "storage_account_name = \"coclico\"\n",
    "storage_options = {\"account_name\": storage_account_name, \"sas_token\": sas_token}\n",
    "\n",
    "LOWER_THAN = (\n",
    "    5  # threshold in meter to compute \"xx of the 1-km coastal zone is <LOWER_THAN> m.\"\n",
    ")\n",
    "H3_LEVEL = 7  # h3 hexagonal-grid zoom-level to visualize the results\n",
    "MERCANTILES_LEVEL = 4\n",
    "\n",
    "COMPUTE_GCTS_LANDWARD = False\n",
    "COMPUTE_GCTS_ELEVATION = False\n",
    "COMPUTE_H3_ELEVATION_STATISTICS = False\n",
    "COMPUTE_GLOBAL_LT_STATISTIC = False\n",
    "\n",
    "GCTS_LANDWARD_CONTAINER = \"az://public/coastal-analytics/gcts-2000m-landward.parquet\"\n",
    "GCTS_ELEVATION_CONTAINER = \"az://public/coastal-analytics/gcts-2000m-elevation.parquet\"\n",
    "H3_ELEVATION_CONTAINER = (\n",
    "    f\"az://public/coastal-analytics/h3-l{H3_LEVEL}-pct-lt-{LOWER_THAN}m.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Connect to the CoCliCo STAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coastpy.stac.utils import read_snapshot\n",
    "\n",
    "# Read the CoCliCo STAC catalog\n",
    "coclico_catalog = pystac.Catalog.from_file(\n",
    "    \"https://coclico.blob.core.windows.net/stac/v1/catalog.json\"\n",
    ")\n",
    "\n",
    "# Read the spatial footprint of the transect partitions.\n",
    "gcts_collection = coclico_catalog.get_child(\"gcts\")\n",
    "gcts_extents = read_snapshot(gcts_collection, columns=[\"geometry\", \"assets\"])\n",
    "\n",
    "# Read the spatial footprint of the DeltaDTM tiles.\n",
    "ddtm_collection = coclico_catalog.get_child(\"deltares-delta-dtm\")\n",
    "ddtm_extents = read_snapshot(ddtm_collection, columns=[\"geometry\", \"assets\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Make a compute cluster\n",
    "\n",
    "This analysis was run on a Dask Gateway at Planetary Computer, but that infrastructure has been retired, so now we use a LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed.client import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Part 1: Compute the landward part of the cross-shore transects\n",
    "\n",
    "The cross-shore coastal transects are 2km long, extending 1km in both land and see-ward direction. The statistics are computed over the 1km landward side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if COMPUTE_GCTS_LANDWARD:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    def map_extract_landward_side(df):\n",
    "        def extract_landward_side(row):\n",
    "            p1 = shapely.Point(row.geometry.coords[0])\n",
    "            p2 = shapely.Point(row.lon, row.lat)\n",
    "            return shapely.LineString([p1, p2])\n",
    "\n",
    "        geoms = df.apply(extract_landward_side, axis=1)\n",
    "        return gpd.GeoDataFrame(df[\"transect_id\"], geometry=geoms, crs=4326)\n",
    "\n",
    "    gcts_collection = coclico_catalog.get_child(\"gcts\")\n",
    "    gcts_extents = read_snapshot(gcts_collection, columns=[\"geometry\", \"assets\"])\n",
    "    gcts_hrefs = gcts_extents.href.to_list()\n",
    "\n",
    "    # template GDF that matches what is retunred from map_extract_landward_side\n",
    "    META = gpd.GeoDataFrame(\n",
    "        {\n",
    "            \"transect_id\": gpd.GeoSeries([], dtype=str),\n",
    "            \"geometry\": gpd.GeoSeries([], dtype=GeometryDtype()),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    transects = dask_geopandas.read_parquet(\n",
    "        gcts_hrefs,\n",
    "        storage_options=storage_options,\n",
    "        columns=[\"transect_id\", \"geometry\", \"lon\", \"lat\"],\n",
    "    )\n",
    "\n",
    "    transects = transects.map_partitions(map_extract_landward_side, meta=META)\n",
    "    transects.to_parquet(GCTS_LANDWARD_CONTAINER, storage_options=storage_options)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time (H:M:S): {time.strftime('%H:%M:%S', time.gmtime(elapsed_time))}\")\n",
    "    client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Part 2: Extract elevation data per transect\n",
    "\n",
    "1) First filter out the files that have already been processed\n",
    "2) Extract the elevation data per transect for each Deltares DeltaDTM tile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if COMPUTE_GCTS_ELEVATION:\n",
    "\n",
    "    from dask.distributed import TimeoutError\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    def to_out_href(href, out_prefix):\n",
    "        return out_prefix + \"/\" + href.split(\"_\")[-1].split(\".\")[0] + \".parquet\"\n",
    "\n",
    "    @dask.delayed\n",
    "    def extract_by_geometry(href, transects, storage_options):\n",
    "        \"\"\"Extract elevation raster data by transect LineString geometry in columnar format.\"\"\"\n",
    "\n",
    "        da = rioxarray.open_rasterio(href, chunks={}, lock=False).squeeze().drop(\"band\")\n",
    "\n",
    "        bbox = da.rio.bounds()\n",
    "        bbox = gpd.GeoDataFrame(geometry=[shapely.box(*bbox)], crs=4326)\n",
    "\n",
    "        transects = gpd.overlay(transects, bbox)\n",
    "\n",
    "        if transects.empty:\n",
    "            return\n",
    "\n",
    "        da = da.where(da != da.rio.nodata, np.nan)\n",
    "        da = da.rio.write_nodata(np.nan)\n",
    "\n",
    "        # TODO: ensure that transect_id is tracked so that we can use the elevation data later at a transect level\n",
    "        clipped = da.rio.clip(transects.geometry.to_list()).rename(\"band_data\")\n",
    "\n",
    "        df = (\n",
    "            clipped.drop([\"spatial_ref\"])\n",
    "            .to_dataframe()\n",
    "            .dropna()\n",
    "            .reset_index()\n",
    "            .rename(columns={\"x\": \"lng\", \"y\": \"lat\"})\n",
    "            .rename(columns={\"band_data\": \"z\"})[[\"lng\", \"lat\", \"z\"]]\n",
    "        )\n",
    "\n",
    "        out_href = to_out_href(href, GCTS_ELEVATION_CONTAINER)\n",
    "\n",
    "        with fsspec.open(out_href, \"wb\", **storage_options) as f:\n",
    "            df.to_parquet(f)\n",
    "\n",
    "    ddtm_collection = coclico_catalog.get_child(\"deltares-delta-dtm\")\n",
    "    ddtm_extents = read_snapshot(ddtm_collection, columns=[\"geometry\", \"assets\"])\n",
    "\n",
    "    tiles = make_mercantiles(MERCANTILES_LEVEL)\n",
    "\n",
    "    ddtm_extents = (\n",
    "        gpd.sjoin(ddtm_extents, tiles)\n",
    "        .drop(columns=\"index_right\")\n",
    "        .sample(frac=1)\n",
    "        .drop_duplicates(\"href\", keep=\"first\")\n",
    "        .sort_index()\n",
    "    )\n",
    "    ddtm_extents[\"out_href\"] = ddtm_extents.href.map(\n",
    "        lambda href: to_out_href(href, GCTS_ELEVATION_CONTAINER)\n",
    "    )\n",
    "\n",
    "    fs = fsspec.filesystem(\"az\", **storage_options)\n",
    "    processed_files = fs.glob(f\"{GCTS_ELEVATION_CONTAINER}/*.parquet\")\n",
    "    processed_files = [f\"az://{f}\" for f in processed_files]\n",
    "\n",
    "    ddtm_extents = ddtm_extents.loc[~ddtm_extents[\"out_href\"].isin(processed_files)]\n",
    "    print(f\"Number of DeltaDTM tiles to process: {len(ddtm_extents)}\")\n",
    "\n",
    "    for name, gr in ddtm_extents.groupby(\"quadkey\"):\n",
    "        print(\n",
    "            f\"Start processing quadkey {name}, with total bounds: {gr.total_bounds}, that in total consist of {len(gr)} DeltaDTM tiles.\"\n",
    "        )\n",
    "\n",
    "        roi = gpd.GeoDataFrame(geometry=[shapely.box(*gr.total_bounds)], crs=4326)\n",
    "        ddtm_hrefs = gr.href.to_list()\n",
    "\n",
    "        client.restart(wait_for_workers=False)\n",
    "\n",
    "        try:\n",
    "            transects = (\n",
    "                dask_geopandas.read_parquet(\n",
    "                    GCTS_LANDWARD_CONTAINER, storage_options=storage_options\n",
    "                )\n",
    "                .sjoin(roi)\n",
    "                .drop(columns=[\"index_right\"])\n",
    "                .compute(timeout=\"5m\")\n",
    "            )\n",
    "        except TimeoutError:\n",
    "            print(\"Loading transects timed out after 5 minutes.\")\n",
    "            continue\n",
    "\n",
    "        transects_scattered = client.scatter(transects, broadcast=True)\n",
    "\n",
    "        tasks = []\n",
    "        for href in ddtm_hrefs:\n",
    "            href = f\"https://coclico.blob.core.windows.net/{href.strip('az://')}?{sas_token}\"\n",
    "            tasks.append(\n",
    "                extract_by_geometry(href, transects_scattered, storage_options)\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            dask.compute(\n",
    "                *tasks, timeout=\"8m\"\n",
    "            )  # 8 minutes timeout for the whole operation\n",
    "        except TimeoutError:\n",
    "            print(\"The operation timed out after 8 minutes.\")\n",
    "            continue\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time (H:M:S): {time.strftime('%H:%M:%S', time.gmtime(elapsed_time))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Part 3: Computing statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Part 3.1: computing statistics by binning into H3 hexagons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_H3_ELEVATION_STATISTICS:\n",
    "    import h3pandas\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    gr_key = f\"h3_{H3_LEVEL:02d}\"\n",
    "\n",
    "    DTYPES = {\n",
    "        f\"{gr_key}\": \"object\",\n",
    "        f\"pct_lt_{LOWER_THAN}m\": \"int32\",\n",
    "        \"n_obs\": \"int32\",\n",
    "        \"geometry\": GeometryDtype(),\n",
    "        \"lon\": \"float32\",\n",
    "        \"lat\": \"float32\",\n",
    "    }\n",
    "\n",
    "    # Create META using DTYPES\n",
    "    META = pd.DataFrame({col: pd.Series(dtype=dt) for col, dt in DTYPES.items()})\n",
    "\n",
    "    def lower_than(group, threshold):\n",
    "        count = (group[\"z\"] < threshold).sum()\n",
    "        total_count = group[\"z\"].count()\n",
    "        percentage = (count / total_count) * 100\n",
    "        return pd.Series(\n",
    "            [percentage, total_count], index=[f\"pct_lt_{threshold}m\", \"n_obs\"]\n",
    "        )\n",
    "\n",
    "    def add_h3(df, level, threshold, meta):\n",
    "        import h3pandas\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        if df.empty:\n",
    "            # Ensure the empty DataFrame matches the META structure\n",
    "            return pd.DataFrame([], columns=meta.columns).astype(meta.dtypes)\n",
    "\n",
    "        df = df.h3.geo_to_h3(level)\n",
    "\n",
    "        # Convert level to string with leading zeros if necessary\n",
    "        level_str = f\"{level:02d}\"  # Ensures two digits\n",
    "        h3_group_key = f\"h3_{level_str}\"\n",
    "        result = (\n",
    "            df.drop(columns=[\"lng\", \"lat\"])\n",
    "            .groupby(h3_group_key)\n",
    "            .apply(lambda gr: lower_than(gr, threshold))\n",
    "            .reset_index()\n",
    "        )\n",
    "        result = result.set_index(h3_group_key).h3.h3_to_geo_boundary().reset_index()\n",
    "\n",
    "        points = result.set_index(\"h3_07\").h3.h3_to_geo().geometry\n",
    "        result[\"lon\"] = points.x.values\n",
    "        result[\"lat\"] = points.y.values\n",
    "\n",
    "        # Ensure the result matches the META structure and data types\n",
    "        result = result.astype(meta.dtypes.to_dict())\n",
    "\n",
    "        return result\n",
    "\n",
    "    ddf = dd.read_parquet(\n",
    "        GCTS_ELEVATION_CONTAINER + \"/*.parquet\",\n",
    "        storage_options=storage_options,\n",
    "    )\n",
    "\n",
    "    ddf = ddf.map_partitions(\n",
    "        lambda df: add_h3(df, level=H3_LEVEL, threshold=LOWER_THAN, meta=META),\n",
    "        meta=META,\n",
    "    )\n",
    "    df = ddf.compute().reset_index(drop=True)\n",
    "\n",
    "    with fsspec.open(H3_ELEVATION_CONTAINER, mode=\"wb\", **storage_options) as f:\n",
    "        df.to_parquet(f)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time (H:M:S): {time.strftime('%H:%M:%S', time.gmtime(elapsed_time))}\")\n",
    "\n",
    "    client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Part 3.2: Compute global statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "if COMPUTE_GLOBAL_LT_STATISTIC:\n",
    "\n",
    "    ddf = dd.read_parquet(\n",
    "        GCTS_ELEVATION_CONTAINER + \"/*.parquet\", storage_options=storage_options\n",
    "    )\n",
    "\n",
    "    pct_lt_5m = ((ddf[\"z\"] < LOWER_THAN).mean() * 100).compute()\n",
    "    print(\n",
    "        f\"We find that {round(pct_lt_5m)}% is lower than {LOWER_THAN}m in the 1km coastal zone.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Part 4: Visualization\n",
    "\n",
    "For the visualization we mostly use Holoviews with a Bokeh backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import colorcet as cc\n",
    "import geopandas as gpd\n",
    "import geoviews as gv\n",
    "import h3pandas\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import panel as pn\n",
    "from bokeh.io import export_svg\n",
    "from cartopy import crs as ccrs\n",
    "from holoviews import opts\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Part 4.1: Read the elevation statistics from cloud object store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fsspec.open(H3_ELEVATION_CONTAINER, mode=\"rb\", **storage_options) as f:\n",
    "    h3_elevation = gpd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Part 4.2: Aggregate data on lower H3 grid to avoid overplotting\n",
    "\n",
    "Additionaly we filter out the H3 hexagon's that cross antimerdian boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosses_antimeridian(geom):\n",
    "    \"\"\"\n",
    "    Checks if a geometry crosses the anti-meridian.\n",
    "    \"\"\"\n",
    "    longitudes = [point[0] for point in geom.exterior.coords]\n",
    "    return any(\n",
    "        abs(longitudes[i] - longitudes[i - 1]) > 180 for i in range(1, len(longitudes))\n",
    "    )\n",
    "\n",
    "\n",
    "def to_h3_agg(df, zoom):\n",
    "    h3_agg = (\n",
    "        df[[\"h3_07\", \"pct_lt_5m\"]]\n",
    "        .set_index(\"h3_07\")\n",
    "        .h3.h3_to_parent(zoom)\n",
    "        .set_index(f\"h3_0{zoom}\")\n",
    "        .groupby(f\"h3_0{zoom}\")\n",
    "        .mean()\n",
    "        .h3.h3_to_geo_boundary()\n",
    "    )\n",
    "\n",
    "    h3_agg[\"crosses_antimeridian\"] = (\n",
    "        h3_agg[\"geometry\"].astype(object).apply(crosses_antimeridian)\n",
    "    )\n",
    "    h3_agg = h3_agg.loc[h3_agg[\"crosses_antimeridian\"] == False]\n",
    "    return h3_agg\n",
    "\n",
    "\n",
    "zoom = 3\n",
    "h3_agg = to_h3_agg(h3_elevation, zoom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Part 4.3: Plot global distribution of coastal land that lower than 5 meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_lt_5m_plot = h3_agg[[\"pct_lt_5m\", \"geometry\"]].hvplot(\n",
    "    geo=True,\n",
    "    cmap=cc.CET_L20[::-1],\n",
    "    color=\"pct_lt_5m\",\n",
    "    size=8,\n",
    "    projection=ccrs.Robinson(),\n",
    "    features=[\"land\"],\n",
    "    alpha=0.75,\n",
    "    colorbar=False,\n",
    "    width=539,  # Env. modeling fig instructions\n",
    "    global_extent=True,\n",
    ")\n",
    "\n",
    "global_lt_5m_plot.opts(\n",
    "    hv.opts.Polygons(\n",
    "        # colorbar_position=\"left\",\n",
    "        line_color=None,\n",
    "        colorbar_opts={\n",
    "            \"title\": None,\n",
    "            # \"title\": \"Lower than 5 m (%)\",\n",
    "            # \"orientation\": \"vertical\",\n",
    "            # \"width\": 15,\n",
    "            # \"height\": 400,\n",
    "            \"scale_alpha\": 1,\n",
    "        },\n",
    "        toolbar=\"disable\",\n",
    "    )\n",
    ")\n",
    "\n",
    "grid = gv.feature.grid(\n",
    "    projection=ccrs.Robinson(), title=\"\", fill_color=\"none\", color=\"gray\"\n",
    ")\n",
    "\n",
    "global_lt_5m_plot = global_lt_5m_plot * grid\n",
    "global_lt_5m_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### Export as SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "H3_GLOBAL_LT_5M_FP = (\n",
    "    f\"az://figures/enabling-coastal-analytics/h3_0{zoom}-global-lt-5m.svg\"\n",
    ")\n",
    "\n",
    "renderer = hv.renderer(\"bokeh\")\n",
    "bokeh_figure = renderer.get_plot(global_lt_5m_plot).state\n",
    "bokeh_figure.output_backend = \"svg\"\n",
    "bokeh_figure.background_fill_color = None\n",
    "bokeh_figure.border_fill_color = None\n",
    "\n",
    "# outpath = (pathlib.Path(\"~\") / H3_GLOBAL_LT_5M_FP.strip(\"az://\")).expanduser()\n",
    "# outpath.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# with outpath.open(mode=\"w\") as fp:\n",
    "#     export_svg(bokeh_figure, filename=fp.name)\n",
    "\n",
    "#     # Write to cloud storage\n",
    "#     with fsspec.open(H3_GLOBAL_LT_5M_FP, mode=\"wb\", **storage_options) as cloud_file:\n",
    "#         with open(fp.name, mode=\"rb\") as fp_read:\n",
    "#             cloud_file.write(fp_read.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Part 4.4: Hotspot analysis using spatial correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esda import Moran_Local\n",
    "from libpysal import weights\n",
    "\n",
    "\n",
    "def add_lisa(gdf):\n",
    "    W = weights.Queen.from_dataframe(gdf)\n",
    "    W.transform = \"r\"\n",
    "\n",
    "    # LISA analysis\n",
    "    lisa = Moran_Local(gdf[\"pct_lt_5m\"].values, W)\n",
    "\n",
    "    # Identifying significant hotspots\n",
    "    gdf[\"lisa_q\"] = (\n",
    "        lisa.q\n",
    "    )  # The quadrant they belong to; 1 (HH), 2 (LH), 3 (LL), 4 (HL)\n",
    "    gdf[\"p_val\"] = lisa.p_sim  # P-values for assessing significance\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def format_hotspot_df(df):\n",
    "    def compute_area(df, utm_epsg):\n",
    "        df = df.to_crs(utm_epsg)\n",
    "        return df.geometry.area\n",
    "\n",
    "    with fsspec.open(\n",
    "        \"https://coclico.blob.core.windows.net/public/utm_grid.parquet\"\n",
    "    ) as f:\n",
    "        utm_grid = gpd.read_parquet(f)\n",
    "\n",
    "    df = df.dissolve().explode().reset_index(drop=True)\n",
    "\n",
    "    df[\"utm_epsg\"] = gpd.sjoin(\n",
    "        df.geometry.representative_point().to_frame(\"geometry\"), utm_grid\n",
    "    )[\"epsg\"]\n",
    "\n",
    "    area = (\n",
    "        df.groupby(\"utm_epsg\")\n",
    "        .apply(lambda gr: compute_area(gr, gr.name))\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"area\"})\n",
    "        .sort_values(\"level_1\")\n",
    "        .reset_index(drop=True)[[\"area\"]]\n",
    "    )\n",
    "\n",
    "    df = df.join(area).sort_values(\"area\", ascending=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "h3_agg2 = to_h3_agg(h3_elevation, 5)\n",
    "h3_agg2 = add_lisa(h3_agg2)\n",
    "\n",
    "hotspots = h3_agg2[(h3_agg2[\"lisa_q\"] == 1) & (h3_agg2[\"p_val\"] < 0.05)]\n",
    "hotspots = format_hotspot_df(hotspots)\n",
    "hotspots = hotspots.iloc[: int(len(hotspots) * 0.025)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_plot = global_lt_5m_plot * hotspots.assign(\n",
    "    geometry=hotspots.representative_point()\n",
    ").hvplot.points(\n",
    "    geo=True,\n",
    "    size=\"area\",\n",
    "    color=\"red\",\n",
    "    scale=0.00003,\n",
    "    alpha=0.6,\n",
    "    projection=ccrs.Robinson(),\n",
    ")\n",
    "\n",
    "hotspots_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "#### Export hotspots as SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "H3_GLOBAL_LT_5M_HOTSPOTS_FP = (\n",
    "    f\"az://figures/enabling-coastal-analytics/h3_0{zoom}-global-lt-5m-hotspots.svg\"\n",
    ")\n",
    "\n",
    "renderer = hv.renderer(\"bokeh\")\n",
    "bokeh_figure = renderer.get_plot(hotspots_plot).state\n",
    "bokeh_figure.output_backend = \"svg\"\n",
    "bokeh_figure.background_fill_color = None\n",
    "bokeh_figure.border_fill_color = None\n",
    "\n",
    "# outpath = (pathlib.Path(\"~\") / H3_GLOBAL_LT_5M_HOTSPOTS_FP.strip(\"az://\")).expanduser()\n",
    "# outpath.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# with outpath.open(mode=\"w\") as fp:\n",
    "#     export_svg(bokeh_figure, filename=fp.name)\n",
    "\n",
    "#     # Write to cloud storage\n",
    "#     with fsspec.open(H3_GLOBAL_LT_5M_FP, mode=\"wb\", **storage_options) as cloud_file:\n",
    "#         with open(fp.name, mode=\"rb\") as fp_read:\n",
    "#             cloud_file.write(fp_read.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Part 4:5: Violin plot of the global coastal elevation in the first km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fsspec.open(H3_ELEVATION_CONTAINER, mode=\"rb\", **storage_options) as f:\n",
    "    h3_elevation = gpd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def width_in_pixels(mm, dpi):\n",
    "    return int((mm / 25.4) * dpi)\n",
    "\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "\n",
    "# Assuming h3_elevation is a pandas DataFrame with the column 'pct_lt_5m' containing the data\n",
    "violin_plot = hv.Violin(h3_elevation[[\"pct_lt_5m\"]], vdims=\"pct_lt_5m\")\n",
    "\n",
    "# Configure the plot options\n",
    "violin_plot = violin_plot.opts(\n",
    "    opts.Violin(\n",
    "        inner=\"quartiles\",\n",
    "        cmap=cc.CET_L20,\n",
    "        # yticks=[(i, f\"{i}\") for i in np.arange(0, 101, 20)],\n",
    "        # xticks=None,\n",
    "        ylabel=\"\",\n",
    "        # fontsize={\"ticks\": 9},\n",
    "        width=width_in_pixels(mm=140 * (1 / 3), dpi=97),\n",
    "        cut=0,  # clips violin to (0,100)\n",
    "        border=0,\n",
    "        padding=0,\n",
    "        show_frame=False,\n",
    "        yaxis=\"right\",\n",
    "    )\n",
    ")\n",
    "\n",
    "violin_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### Export to svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "H3_GLOBAL_DISTRIBUTION_LT_5M = (\n",
    "    f\"az://figures/enabling-coastal-analytics/h3_0{zoom}-global-distribution-lt-5m.svg\"\n",
    ")\n",
    "\n",
    "outpath = (pathlib.Path(\"~\") / H3_GLOBAL_DISTRIBUTION_LT_5M.strip(\"az://\")).expanduser()\n",
    "outpath.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "renderer = hv.renderer(\"bokeh\")\n",
    "bokeh_figure = renderer.get_plot(violin_plot).state\n",
    "bokeh_figure.output_backend = \"svg\"\n",
    "# bokeh_figure.background_fill_color = None\n",
    "# bokeh_figure.border_fill_color = None\n",
    "# bokeh_figure.outline_line_color = None\n",
    "# bokeh_figure.outline_line_width = 0\n",
    "\n",
    "\n",
    "# with outpath.open(mode=\"w\") as fp:\n",
    "#     export_svg(bokeh_figure, filename=fp.name)\n",
    "\n",
    "#     # Write to cloud storage\n",
    "#     with fsspec.open(H3_GLOBAL_LT_5M_FP, mode=\"wb\", **storage_options) as cloud_file:\n",
    "#         with open(fp.name, mode=\"rb\") as fp_read:\n",
    "#             cloud_file.write(fp_read.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Part 5: Figure with example of high-resolution data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import duckdb\n",
    "import geoviews.tile_sources as gvts\n",
    "import holoviews as hv\n",
    "import hvplot.pandas  # Ensures hvplot is available\n",
    "from bokeh.io import export_png\n",
    "from holoviews import opts\n",
    "from holoviews.operation.datashader import dynspread\n",
    "from ipyleaflet import Map, basemaps\n",
    "from shapely import wkb\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Part 5.1 Select a region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, basemaps\n",
    "\n",
    "m = Map(basemap=basemaps.Esri.WorldImagery, scroll_wheel_zoom=True)\n",
    "m.center = 15.825, -95.95\n",
    "m.zoom = 14\n",
    "m.layout.height = \"725px\"\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = shapely.geometry.box(m.west, m.south, m.east, m.north)\n",
    "roi = gpd.GeoDataFrame(geometry=[roi], crs=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCTS_CONTAINER = gcts_collection.extra_fields[\"base_url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Part 5.2 Extract the data using DuckDB\n",
    "\n",
    "We do not have STACs for all data that was produced in this experiment, so we just use DuckDB to efficiently filter the container by predicate pushdown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the bounding box coordinates from the region of interest (ROI)\n",
    "minx, miny, maxx, maxy = roi.total_bounds\n",
    "\n",
    "# Connect to an in-memory DuckDB instance and load necessary extensions\n",
    "con = duckdb.connect(database=\":memory:\", read_only=False)\n",
    "con.execute(\"INSTALL azure;\")\n",
    "con.execute(\"LOAD azure;\")\n",
    "con.execute(\"INSTALL spatial;\")\n",
    "con.execute(\"LOAD spatial;\")\n",
    "\n",
    "# Create a secret for Azure connection\n",
    "try:\n",
    "    con.execute(\n",
    "        f\"\"\"\n",
    "        CREATE SECRET azure_secret (\n",
    "        TYPE AZURE,\n",
    "        CONNECTION_STRING '{os.getenv('CLIENT_AZURE_STORAGE_CONNECTION_STRING')}');\n",
    "        \"\"\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"The secret is likely already available. Exception message:\\n\\n{e}\")\n",
    "\n",
    "# Define the query to read transect data within the bounding box\n",
    "transect_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_parquet('{GCTS_CONTAINER + \"/*.parquet\"}')\n",
    "    WHERE\n",
    "        bbox.xmin <= {maxx} AND\n",
    "        bbox.ymin <= {maxy} AND\n",
    "        bbox.xmax >= {minx} AND\n",
    "        bbox.ymax >= {miny};\n",
    "\"\"\"\n",
    "\n",
    "# Execute the transect query and fetch the result as a DataFrame\n",
    "transect_df = con.execute(transect_query).fetchdf()\n",
    "\n",
    "# Convert ROI geometry to WKT format\n",
    "roi_wkt = roi.geometry.to_wkt().iloc[0]\n",
    "\n",
    "# Create or replace the transects table and fetch the intersecting transects\n",
    "con.execute(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE transects AS \n",
    "    SELECT * FROM read_parquet('{GCTS_LANDWARD_CONTAINER + \"/*.parquet\"}');\n",
    "    \"\"\"\n",
    ")\n",
    "landward_transects_df = con.execute(\n",
    "    f\"\"\"\n",
    "    SELECT * \n",
    "    FROM transects \n",
    "    WHERE ST_Intersects(ST_GeomFromWKB(transects.geometry), ST_GeomFromText('{roi_wkt}'));\n",
    "    \"\"\"\n",
    ").fetchdf()\n",
    "\n",
    "# Define the query to read elevation data within the bounding box\n",
    "elevation_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_parquet('{GCTS_ELEVATION_CONTAINER + \"/*.parquet\"}')\n",
    "    WHERE\n",
    "        lng <= {maxx} AND\n",
    "        lat <= {maxy} AND\n",
    "        lng >= {minx} AND\n",
    "        lat >= {miny};\n",
    "\"\"\"\n",
    "\n",
    "# Execute the elevation query and fetch the result as a DataFrame\n",
    "elevation_df = con.execute(elevation_query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "transect_gdf = gpd.GeoDataFrame(\n",
    "    transect_df,\n",
    "    geometry=transect_df.geometry.apply(lambda x: wkb.loads(bytes(x))),\n",
    "    crs=4326,\n",
    ")\n",
    "\n",
    "landward_transects_gdf = gpd.GeoDataFrame(\n",
    "    landward_transects_df,\n",
    "    geometry=landward_transects_df.geometry.apply(lambda x: wkb.loads(bytes(x))),\n",
    "    crs=4326,\n",
    ")\n",
    "\n",
    "elevation_gdf = gpd.GeoDataFrame(\n",
    "    elevation_df,\n",
    "    geometry=gpd.points_from_xy(elevation_df.lng, elevation_df.lat, crs=4326),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "#### Plot the elevation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_plot = elevation_gdf.hvplot(\n",
    "    geo=True, datashade=True, color=\"z\", cmap=cc.CET_L20, colorbar=True, width=800\n",
    ")\n",
    "dynspread(elevation_plot, threshold=0.5, max_px=200) * gvts.EsriImagery()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Part 5.2: Adjust the area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_m = Map(basemap=basemaps.Esri.WorldImagery, scroll_wheel_zoom=True)\n",
    "inner_m.center = 15.827, -95.96\n",
    "\n",
    "inner_m.zoom = 15\n",
    "inner_m.layout.height = \"725px\"\n",
    "inner_m.layout.width = \"725px\"\n",
    "inner_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_roi = shapely.geometry.box(\n",
    "    inner_m.west, inner_m.south, inner_m.east, inner_m.north\n",
    ")\n",
    "inner_roi_gdf = gpd.GeoDataFrame(geometry=[inner_roi], crs=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "transect_gdf_inner = transect_gdf.iloc[\n",
    "    gpd.sjoin(\n",
    "        gpd.GeoSeries.from_xy(transect_gdf.lon, transect_gdf.lat, crs=4326).to_frame(\n",
    "            \"geometry\"\n",
    "        ),\n",
    "        inner_roi_gdf,\n",
    "    ).index.to_list()\n",
    "]\n",
    "\n",
    "landward_transects_gdf_inner = gpd.sjoin(\n",
    "    landward_transects_gdf, transect_gdf_inner[[\"geometry\"]]\n",
    ").drop(columns=[\"index_right\"])\n",
    "\n",
    "# Create a buffer so that we only show elevation data at transects\n",
    "landward_transect_buffer = landward_transects_gdf_inner.to_crs(\n",
    "    landward_transects_gdf_inner.estimate_utm_crs()\n",
    ").buffer(50)\n",
    "\n",
    "elevation_gdf_inner = (\n",
    "    gpd.sjoin(\n",
    "        elevation_gdf.reset_index(),\n",
    "        landward_transect_buffer.to_crs(4326).to_frame(\"geometry\"),\n",
    "    )\n",
    "    .drop_duplicates(\"index\")\n",
    "    .drop(columns=[\"index\", \"index_right\"])\n",
    ")\n",
    "\n",
    "# transect_gdf_inner_bounds = gpd.GeoDataFrame(\n",
    "#     geometry=[shapely.geometry.box(*transect_gdf_inner.total_bounds)], crs=4326\n",
    "# )\n",
    "\n",
    "# elevation_gdf_inner = gpd.sjoin(elevation_gdf, transect_gdf_inner_bounds).drop(\n",
    "#     columns=[\"index_right\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### Part 5.3 Create the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_plot = gvts.EsriImagery() * (\n",
    "    transect_gdf_inner[[\"geometry\"]].hvplot(\n",
    "        geo=True,\n",
    "        width=1200,\n",
    "        label=\"GCTS - seaward\",\n",
    "        xlabel=\"Longitude\",\n",
    "        ylabel=\"Latitude\",\n",
    "        line_width=4,\n",
    "        fontscale=3,\n",
    "    )\n",
    "    * elevation_gdf_inner.hvplot(\n",
    "        geo=True,\n",
    "        cmap=cc.CET_L20,\n",
    "        color=\"z\",\n",
    "        colorbar=True,\n",
    "        clabel=\"DeltaDTM elevation (m)\",\n",
    "        fontscale=3,\n",
    "        size=200,\n",
    "    )\n",
    "    * landward_transects_gdf_inner.hvplot(\n",
    "        geo=True,\n",
    "        color=\"red\",\n",
    "        alpha=1,\n",
    "        label=\"GCTS - landward\",\n",
    "        fontscale=3,\n",
    "        line_width=4,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Apply opts to customize legends and colorbar\n",
    "hr_plot.opts(\n",
    "    hv.opts.Overlay(\n",
    "        legend_position=\"bottom_right\",  # Position the legend at the bottom right\n",
    "        toolbar=\"disable\",  # Toolbar position can be adjusted here if needed\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the final plot\n",
    "pn.Column(hr_plot).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### Export figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "H3_HR_LT_5M = (\n",
    "    f\"az://figures/enabling-coastal-analytics/hr-data-extract-barra-de-la-cruz.png\"\n",
    ")\n",
    "\n",
    "outpath = (pathlib.Path(\"~\") / H3_HR_LT_5M.strip(\"az://\")).expanduser()\n",
    "outpath.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "renderer = hv.renderer(\"bokeh\")\n",
    "bokeh_figure = renderer.get_plot(hr_plot).state\n",
    "# bokeh_figure.output_backend = \"png\"\n",
    "bokeh_figure.background_fill_color = None\n",
    "bokeh_figure.border_fill_color = None\n",
    "\n",
    "\n",
    "# with outpath.open(mode=\"w\") as fp:\n",
    "#     export_png(bokeh_figure, filename=fp.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coastal-full] *",
   "language": "python",
   "name": "conda-env-coastal-full-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
