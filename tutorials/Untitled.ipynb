{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import geopandas as gpd\n",
    "import hvplot.xarray\n",
    "import pystac\n",
    "import rioxarray\n",
    "import shapely\n",
    "import xarray as xr\n",
    "from ipyleaflet import Map, basemaps\n",
    "\n",
    "from coastpy.stac.utils import read_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Map(basemap=basemaps.Esri.WorldImagery, scroll_wheel_zoom=True)\n",
    "m.center = 46.34, -1.47\n",
    "m.zoom = 15\n",
    "m.layout.height = \"800px\"\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fsspec.open(\n",
    "    \"https://coclico.blob.core.windows.net/tiles/S2A_OPER_GIP_TILPAR_MPC.parquet\", \"rb\"\n",
    ") as f:\n",
    "    s2grid = gpd.read_parquet(f).to_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "sas_token = os.getenv(\"AZURE_STORAGE_SAS_TOKEN\")\n",
    "storage_options = {\"account_name\": \"coclico\", \"sas_token\": sas_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import fsspec\n",
    "import geopandas as gpd\n",
    "\n",
    "from coastpy.geo.quadtiles import make_mercantiles\n",
    "\n",
    "grid = make_mercantiles(5)\n",
    "\n",
    "# Load coastline buffer data (2km buffer around the coastline) and convert to WGS84\n",
    "buffer = dask_geopandas.read_parquet(\n",
    "    \"az://coastline-buffer/osm-coastlines-buffer-2000m.parquet\",\n",
    "    storage_options=storage_options,\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script to generate a processing grid for scalable geospatial analysis.\n",
    "This script:\n",
    "1. Generates a Mercator grid based on a specified zoom level.\n",
    "2. Reads and processes coastline buffer data.\n",
    "3. Clips the buffer to the European region.\n",
    "4. Filters tiles based on intersection with the coastline.\n",
    "5. # TODO: Clips tiles based on a coastline buffer.  \n",
    "\n",
    "Dependencies: dask-geopandas, geopandas, fsspec\n",
    "\"\"\"\n",
    "\n",
    "import dask_geopandas as dgpd\n",
    "import fsspec\n",
    "import geopandas as gpd\n",
    "\n",
    "from coastpy.geo.quadtiles import make_mercantiles\n",
    "\n",
    "grid = make_mercantiles(5)\n",
    "\n",
    "# Load coastline buffer data (2km buffer around the coastline) and convert to WGS84\n",
    "buffer = dask_geopandas.read_parquet(\n",
    "    \"az://coastline-buffer/osm-coastlines-buffer-2000m.parquet\",\n",
    "    storage_options=storage_options,\n",
    ").compute()\n",
    "\n",
    "\n",
    "# Load coastline data\n",
    "coastline_urlpath = \"az://coastlines-osm/release/2023-02-09/coast_3857_gen9.parquet\"\n",
    "with fsspec.open(coastline_urlpath, mode=\"rb\", **storage_options) as f:\n",
    "    coastline = gpd.read_parquet(f).to_crs(4326)\n",
    "\n",
    "\n",
    "# Load countries dataset and filter for Europe\n",
    "with fsspec.open(\n",
    "    \"https://coclico.blob.core.windows.net/public/countries.parquet\", \"rb\"\n",
    ") as f:\n",
    "    countries = gpd.read_parquet(f)\n",
    "\n",
    "europe = countries[\n",
    "    (countries[\"continent\"] == \"EU\")\n",
    "    & (~countries[\"common_country_name\"].isin([\"Svalbard\", \"Russia\"]))\n",
    "]\n",
    "\n",
    "# Clip the coastline buffer to the European region\n",
    "european_coastline = gpd.clip(coastline, europe)\n",
    "\n",
    "# Filter the Mercator grid tiles by spatial join with the European buffer\n",
    "filtered_tiles = grid.sjoin(european_coastline, how=\"inner\")\n",
    "\n",
    "# Drop unnecessary columns to clean the output\n",
    "filtered_tiles = filtered_tiles.drop(columns=[\"index_right\"])\n",
    "\n",
    "# TODO: I want the tiles to be clipped by the buffer. So that within each tile\n",
    "# I only have the buffer area (coastal zone)\n",
    "...  # < your code comes here.\n",
    "clipped_tiles = gpd.overlay(filtered_tiles, european_buffer, how=\"intersection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer_antimeridian(buffer_size):\n",
    "    \"\"\"\n",
    "    Create a buffer around the antimeridian to handle crossings.\n",
    "\n",
    "    Args:\n",
    "        buffer_size (float): Buffer distance in degrees.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: Buffered antimeridian area.\n",
    "    \"\"\"\n",
    "    antimeridian = shapely.geometry.LineString([(180, -90), (180, 90)])\n",
    "    buffer_geom = shapely.geometry.Polygon(antimeridian.buffer(buffer_size))\n",
    "    return gpd.GeoDataFrame(geometry=[buffer_geom], crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "buffer_antimeridian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = gpd.GeoDataFrame(\n",
    "    geometry=[shapely.geometry.LineString([(180, -90), (180, 90)])], crs=4326\n",
    ")\n",
    "\n",
    "\n",
    "utm_northeast = 32660\n",
    "utm_northwest = 32601\n",
    "utm_southeast = ...\n",
    "utm_southwest = ...\n",
    "\n",
    "print(list(geom.to_crs(utm_crs).geometry.item().coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import shapely.geometry\n",
    "from pyproj import CRS, Transformer\n",
    "from shapely.ops import transform\n",
    "\n",
    "\n",
    "def create_antimeridian_buffer(buffer_size_meters):\n",
    "    \"\"\"\n",
    "    Create a valid buffer area around the antimeridian.\n",
    "\n",
    "    Args:\n",
    "        buffer_size_meters (float): Buffer distance in meters.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: Buffered area around the antimeridian in EPSG:4326.\n",
    "    \"\"\"\n",
    "    # Step 1: Define the antimeridian line\n",
    "    antimeridian_line = gpd.GeoDataFrame(\n",
    "        geometry=[shapely.geometry.LineString([(180, -90), (180, 90)])], crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # Step 2: Define UTM zones for each quadrant\n",
    "    utm_zones = {\n",
    "        \"northeast\": 32660,  # UTM zone 60N\n",
    "        \"northwest\": 32601,  # UTM zone 1N\n",
    "        \"southeast\": 32760,  # UTM zone 60S\n",
    "        \"southwest\": 32701,  # UTM zone 1S\n",
    "    }\n",
    "\n",
    "    buffered_quadrants = []\n",
    "\n",
    "    # Step 3 & 4: Transform to UTM, compute buffer, and convert back\n",
    "    for quadrant, utm_crs in utm_zones.items():\n",
    "        transformer_to_utm = Transformer.from_crs(\n",
    "            \"EPSG:4326\", CRS.from_epsg(utm_crs), always_xy=True\n",
    "        )\n",
    "        transformer_to_4326 = Transformer.from_crs(\n",
    "            CRS.from_epsg(utm_crs), \"EPSG:4326\", always_xy=True\n",
    "        )\n",
    "\n",
    "        # Transform the antimeridian line to UTM CRS\n",
    "        utm_line = transform(\n",
    "            transformer_to_utm.transform, antimeridian_line.geometry.item()\n",
    "        )\n",
    "\n",
    "        # Buffer in UTM CRS\n",
    "        utm_buffer = shapely.geometry.Polygon(\n",
    "            [\n",
    "                (utm_line.coords[0][0] - buffer_size_meters, utm_line.coords[0][1]),\n",
    "                (utm_line.coords[1][0] - buffer_size_meters, utm_line.coords[1][1]),\n",
    "                (utm_line.coords[1][0] + buffer_size_meters, utm_line.coords[1][1]),\n",
    "                (utm_line.coords[0][0] + buffer_size_meters, utm_line.coords[0][1]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Convert buffered geometry back to EPSG:4326\n",
    "        epsg4326_buffer = transform(transformer_to_4326.transform, utm_buffer)\n",
    "\n",
    "        # Append to the list of buffered quadrants\n",
    "        buffered_quadrants.append(epsg4326_buffer)\n",
    "\n",
    "    # Step 5: Combine buffered quadrants into one GeoDataFrame\n",
    "    combined_buffers = gpd.GeoDataFrame(geometry=buffered_quadrants, crs=\"EPSG:4326\")\n",
    "\n",
    "    # Step 6: Validate geometries\n",
    "    combined_buffers[\"is_valid\"] = combined_buffers.geometry.is_valid\n",
    "    if not combined_buffers[\"is_valid\"].all():\n",
    "        print(\"Some geometries are invalid. Attempting to fix...\")\n",
    "        combined_buffers[\"geometry\"] = combined_buffers.geometry.apply(\n",
    "            lambda geom: geom.buffer(0) if not geom.is_valid else geom\n",
    "        )\n",
    "\n",
    "    return combined_buffers\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "buffer_size = 300000  # 300 km buffer\n",
    "antimeridian_buffer = create_antimeridian_buffer(buffer_size)\n",
    "print(antimeridian_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "def compute_geodesic_point(lat, lon, distance_km, bearing):\n",
    "    \"\"\"\n",
    "    Compute a geodesic destination point.\n",
    "\n",
    "    Args:\n",
    "        lat (float): Latitude of the starting point.\n",
    "        lon (float): Longitude of the starting point.\n",
    "        distance_km (float): Distance to travel in kilometers.\n",
    "        bearing (float): Direction of travel in degrees (e.g., 90 for east).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (latitude, longitude) of the destination point.\n",
    "    \"\"\"\n",
    "    start_point = (lat, lon)\n",
    "    destination = geodesic(kilometers=distance_km).destination(start_point, bearing)\n",
    "    return destination.latitude, destination.longitude\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "start_lat = 0\n",
    "start_lon = -180\n",
    "distance = 5  # in kilometers\n",
    "bearing = 90  # eastward\n",
    "\n",
    "destination_point = compute_geodesic_point(start_lat, start_lon, distance, bearing)\n",
    "print(f\"Destination Point: {destination_point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lat, start_lon = 0.1, -180  # Antimeridian equator point\n",
    "shifted_lat, shifted_lon = compute_geodesic_point(start_lat, start_lon, 5, 90)\n",
    "\n",
    "p1 = shapely.Point(start_lon, start_lat)\n",
    "p2 = shapely.Point(shifted_lon, shifted_lat)\n",
    "\n",
    "gdf1 = gpd.GeoDataFrame(geometry=[p1], crs=4326)\n",
    "gdf2 = gpd.GeoDataFrame(geometry=[p2], crs=4326)\n",
    "\n",
    "m = gdf1.explore()\n",
    "gdf2.explore(m=m, color=\"red\")\n",
    "\n",
    "utm_crs = gdf2.estimate_utm_crs()\n",
    "\n",
    "line = gpd.GeoDataFrame(\n",
    "    geometry=[shapely.LineString([shapely.Point(p2.x, 0), shapely.Point(p2.x, 84)])],\n",
    "    crs=4326,\n",
    ")\n",
    "line.assign(geometry=line.to_crs(utm_crs).buffer(2500)).to_file(\n",
    "    \"/Users/calkoen/tmp/test.gpkg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import antimeridian\n",
    "from shapely.geometry import shape\n",
    "from shapely.validation import make_valid\n",
    "\n",
    "\n",
    "def correct_antimeridian_cross(row):\n",
    "    \"\"\"\n",
    "    Correct geometries crossing the antimeridian using the antimeridian library.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): Row containing the geometry to correct.\n",
    "\n",
    "    Returns:\n",
    "        shapely.geometry.base.BaseGeometry: Corrected geometry.\n",
    "    \"\"\"\n",
    "    geom = row.geometry\n",
    "\n",
    "    try:\n",
    "        # Convert GeoJSON-like geometries to Shapely if necessary\n",
    "        if isinstance(geom, dict):\n",
    "            geom = shape(geom)\n",
    "\n",
    "        # Ensure geometry is valid\n",
    "        if not geom.is_valid:\n",
    "            geom = make_valid(geom)\n",
    "\n",
    "        # Fix geometry using antimeridian library\n",
    "        return antimeridian.fix_polygon(geom, fix_winding=False)\n",
    "    except Exception as e:\n",
    "        # Log and return the original geometry if correction fails\n",
    "        print(e)\n",
    "        return geom\n",
    "\n",
    "\n",
    "def correct_antimeridian_crosses_in_df(df):\n",
    "    \"\"\"\n",
    "    Correct geometries that cross the antimeridian.\n",
    "\n",
    "    Args:\n",
    "        df (gpd.GeoDataFrame): Input GeoDataFrame with `crosses_antimeridian` column.\n",
    "        utm_grid (gpd.GeoDataFrame): UTM grid for overlay.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: Updated GeoDataFrame with corrected geometries.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Create a boolean mask for rows to correct\n",
    "    rows_to_correct = df[\"crosses_antimeridian\"]\n",
    "\n",
    "    # Apply the correction only to rows where `crosses_antimeridian` is True\n",
    "    df.loc[rows_to_correct, \"geometry\"] = df.loc[rows_to_correct].apply(\n",
    "        lambda row: correct_antimeridian_cross(row), axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "gdf = gpd.read_parquet(\"/Users/calkoen/tmp/data.parquet\")\n",
    "\n",
    "gdf2 = correct_antimeridian_crosses_in_df(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "buf = gpd.read_file(\"/Users/calkoen/data/prc/test_buffer_15000_coast_3857_gen9.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = buf.copy()\n",
    "df[(df.geom_type == \"Polygon\") | (df.geom_type == \"MultiPolygon\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "buf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "buf[buf.geom_type == \"LineString\"].explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosses_antimeridian(geometry):\n",
    "    \"\"\"\n",
    "    Check if a geometry crosses the antimeridian.\n",
    "\n",
    "    Args:\n",
    "        geometry (shapely.geometry.base.BaseGeometry): The input geometry.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the geometry crosses the antimeridian, False otherwise.\n",
    "    \"\"\"\n",
    "    minx, miny, maxx, maxy = geometry.bounds\n",
    "    return maxx - minx > 180\n",
    "\n",
    "\n",
    "def map_crosses_antimeridian(df):\n",
    "    src_crs = df.crs\n",
    "    df = df.to_crs(4326)\n",
    "    df[\"crosses_antimeridian\"] = df[\"geometry\"].apply(crosses_antimeridian)\n",
    "    df = df.to_crs(src_crs)\n",
    "    return df\n",
    "\n",
    "\n",
    "r = map_crosses_antimeridian(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import antimeridian\n",
    "from shapely.validation import make_valid\n",
    "\n",
    "def correct_antimeridian_cross(row):\n",
    "    \"\"\"\n",
    "    Correct geometries crossing the antimeridian using the antimeridian library.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): Row containing the geometry to correct.\n",
    "\n",
    "    Returns:\n",
    "        shapely.geometry.base.BaseGeometry: Corrected geometry.\n",
    "    \"\"\"\n",
    "    geom = row.geometry\n",
    "\n",
    "    try:\n",
    "        # Fix geometry using antimeridian library\n",
    "        import antimeridian\n",
    "\n",
    "        if geom.geom_type == \"Polygon\":\n",
    "            fixed = antimeridian.fix_polygon(geom, fix_winding=True)\n",
    "            fixed = make_valid(fixed)\n",
    "            return fixed\n",
    "        elif geom.geom_type == \"MultiPolygon\":\n",
    "            fixed = antimeridian.fix_multi_polygon(geom, fix_winding=True)\n",
    "            fixed = make_valid(fixed)\n",
    "            return fixed\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def correct_antimeridian_crosses_in_df(df):\n",
    "    \"\"\"\n",
    "    Correct geometries that cross the antimeridian.\n",
    "\n",
    "    Args:\n",
    "        df (gpd.GeoDataFrame): Input GeoDataFrame with `crosses_antimeridian` column.\n",
    "        utm_grid (gpd.GeoDataFrame): UTM grid for overlay.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: Updated GeoDataFrame with corrected geometries.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    crs = df.crs\n",
    "    df = df.to_crs(4326)\n",
    "\n",
    "    # Create a boolean mask for rows to correct\n",
    "    rows_to_correct = df[\"crosses_antimeridian\"]\n",
    "\n",
    "    # Apply the correction only to rows where `crosses_antimeridian` is True\n",
    "    df.loc[rows_to_correct, \"geometry\"] = df.loc[rows_to_correct].apply(\n",
    "        lambda row: correct_antimeridian_cross(row), axis=1\n",
    "    )\n",
    "    df = df.to_crs(crs)\n",
    "    return df\n",
    "\n",
    "r2 = correct_antimeridian_crosses_in_df(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "r3 = r2[r2[\"crosses_antimeridian\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "r3.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "buf.iloc[[299]].explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gdf.explore()\n",
    "gdf2.explore(m=m, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    ")\n",
    "print(gdf.geom_type.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gdf2.explore()\n",
    "gdf.explore(m=m, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from geopy.distance import geodesic\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "\n",
    "def compute_geodesic_point(lat, lon, distance_km, bearing):\n",
    "    start_point = (lat, lon)\n",
    "    destination = geodesic(kilometers=distance_km).destination(start_point, bearing)\n",
    "    return destination.latitude, destination.longitude\n",
    "\n",
    "\n",
    "def create_antimeridian_buffers(buffer_size_km=5):\n",
    "    # Define UTM zones\n",
    "    utm_zones = {\n",
    "        \"northeast\": 32660,  # UTM zone 60N\n",
    "        \"northwest\": 32601,  # UTM zone 1N\n",
    "        \"southeast\": 32760,  # UTM zone 60S\n",
    "        \"southwest\": 32701,  # UTM zone 1S\n",
    "    }\n",
    "\n",
    "    # Initialize storage for polygons\n",
    "    buffered_geometries = []\n",
    "\n",
    "    # Process each quadrant\n",
    "    for quadrant, utm_zone in utm_zones.items():\n",
    "        # Compute east/west starting point\n",
    "        direction = 90 if \"east\" in quadrant else 270  # East for NE/SE, West for NW/SW\n",
    "        start_lat, start_lon = 0, -180  # Antimeridian equator point\n",
    "        shifted_lat, shifted_lon = compute_geodesic_point(\n",
    "            start_lat, start_lon, 5, direction\n",
    "        )\n",
    "\n",
    "        # Create north-south linestring\n",
    "        if \"north\" in quadrant:\n",
    "            line_coords = [(shifted_lon, 0), (shifted_lon, 90)]  # Northward\n",
    "        else:\n",
    "            line_coords = [(shifted_lon, 0), (shifted_lon, -90)]  # Southward\n",
    "\n",
    "        line = LineString(line_coords)\n",
    "\n",
    "        # Convert to UTM, buffer, and back to EPSG:4326\n",
    "        gdf = gpd.GeoDataFrame(geometry=[line], crs=\"EPSG:4326\").to_crs(utm_zone)\n",
    "        gdf[\"geometry\"] = gdf.buffer(buffer_size_km * 1000)  # Buffer in meters\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "        # Store result with metadata\n",
    "        gdf[\"quadrant\"] = quadrant\n",
    "        buffered_geometries.append(gdf)\n",
    "\n",
    "    # Combine all buffered geometries into a single GeoDataFrame\n",
    "    result_gdf = gpd.GeoDataFrame(pd.concat(buffered_geometries, ignore_index=True))\n",
    "\n",
    "    # Validate and fix geometries\n",
    "    result_gdf = result_gdf.set_geometry(\"geometry\")\n",
    "    if not result_gdf.is_valid.all():\n",
    "        result_gdf[\"geometry\"] = result_gdf[\"geometry\"].apply(\n",
    "            lambda geom: geom.buffer(0)\n",
    "        )\n",
    "\n",
    "    return result_gdf\n",
    "\n",
    "\n",
    "# Run the function\n",
    "buffered_antimeridian = create_antimeridian_buffers(buffer_size_km=5)\n",
    "\n",
    "# Visualize the result\n",
    "buffered_antimeridian.explore(tooltip=\"quadrant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "def compute_geodesic_point(lat, lon, distance_km, bearing):\n",
    "    \"\"\"\n",
    "    Compute a geodesic destination point.\n",
    "\n",
    "    Args:\n",
    "        lat (float): Latitude of the starting point.\n",
    "        lon (float): Longitude of the starting point.\n",
    "        distance_km (float): Distance to travel in kilometers.\n",
    "        bearing (float): Direction of travel in degrees (e.g., 90 for east).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (latitude, longitude) of the destination point.\n",
    "    \"\"\"\n",
    "    start_point = (lat, lon)\n",
    "    destination = geodesic(kilometers=distance_km).destination(start_point, bearing)\n",
    "    return destination.latitude, destination.longitude\n",
    "\n",
    "\n",
    "# Compute multiple geodesic points\n",
    "start_lat = 0\n",
    "start_lon = -180\n",
    "distances_km = [5, 10, 15, 20, 25]  # Distances in km\n",
    "bearing = 90  # Eastward\n",
    "\n",
    "# Generate destination points\n",
    "points = [\n",
    "    compute_geodesic_point(start_lat, start_lon, dist, bearing) for dist in distances_km\n",
    "]\n",
    "\n",
    "# Create a DataFrame and GeoDataFrame\n",
    "df = pd.DataFrame(points, columns=[\"latitude\", \"longitude\"])\n",
    "df[\"geometry\"] = [Point(lon, lat) for lat, lon in zip(df[\"latitude\"], df[\"longitude\"])]\n",
    "\n",
    "gdf = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "# Visualize on a map\n",
    "gdf.explore(\n",
    "    tooltip=[\"latitude\", \"longitude\"],\n",
    "    popup=True,\n",
    "    color=\"blue\",\n",
    "    marker_kwds={\"radius\": 5},\n",
    "    title=\"Geodesic Points East of Antimeridian\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "def compute_geodesic_point(start_point, distance_km, bearing_degrees):\n",
    "    \"\"\"\n",
    "    Compute a geodesic point from a starting coordinate traveling a specified\n",
    "    distance in a specified direction.\n",
    "\n",
    "    Args:\n",
    "        start_point (tuple): The starting coordinate (latitude, longitude) as a tuple.\n",
    "        distance_km (float): The distance to travel in kilometers.\n",
    "        bearing_degrees (float): The bearing in degrees (e.g., 90 for east, 270 for west).\n",
    "\n",
    "    Returns:\n",
    "        tuple: The destination coordinate (latitude, longitude).\n",
    "    \"\"\"\n",
    "    # Use geopy to calculate the destination point\n",
    "    destination = geodesic(kilometers=distance_km).destination(\n",
    "        start_point, bearing_degrees\n",
    "    )\n",
    "    return destination.latitude, destination.longitude\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "start_coord = (180, 0)  # Antimeridian, equator\n",
    "distance = 5  # Kilometers\n",
    "bearing = 90  # East\n",
    "\n",
    "new_coord = compute_geodesic_point(start_coord, distance, bearing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = shapely.Point(start_coord)\n",
    "p2 = shapely.Point(new_coord)\n",
    "gpd.GeoDataFrame(geometry=[p1, p2], crs=4326).explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import shapely.geometry\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "\n",
    "def buffer_antimeridian(buffer_size_meters):\n",
    "    \"\"\"\n",
    "    Create a valid buffer around the antimeridian to handle crossings.\n",
    "\n",
    "    Args:\n",
    "        buffer_size_meters (float): Buffer size in meters.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: Buffered antimeridian area as valid polygons.\n",
    "    \"\"\"\n",
    "    # Define UTM zones for east and west of the antimeridian\n",
    "    utm_zone_east = 60  # UTM zone just east of the antimeridian\n",
    "    utm_zone_west = 1  # UTM zone just west of the antimeridian\n",
    "\n",
    "    # Define UTM CRS for each zone\n",
    "    utm_crs_east = CRS.from_epsg(32660)  # Northern hemisphere, UTM zone 60\n",
    "    utm_crs_west = CRS.from_epsg(32601)  # Northern hemisphere, UTM zone 1\n",
    "\n",
    "    # Define the antimeridian in latitude bounds\n",
    "    lat_min, lat_max = -90, 90\n",
    "\n",
    "    # Create transformers to convert EPSG:4326 to UTM and back\n",
    "    to_utm_east = Transformer.from_crs(\"EPSG:4326\", utm_crs_east, always_xy=True)\n",
    "    to_utm_west = Transformer.from_crs(\"EPSG:4326\", utm_crs_west, always_xy=True)\n",
    "    to_epsg4326 = Transformer.from_crs(utm_crs_east, \"EPSG:4326\", always_xy=True)\n",
    "\n",
    "    # Define east buffer polygon in UTM coordinates\n",
    "    east_poly_coords = [\n",
    "        to_utm_east.transform(180, lat_min),  # Start at (180째, lat_min)\n",
    "        to_utm_east.transform(180 + buffer_size_meters / 111320, lat_min),  # East edge\n",
    "        to_utm_east.transform(180 + buffer_size_meters / 111320, lat_max),  # East edge\n",
    "        to_utm_east.transform(180, lat_max),  # Back at (180째, lat_max)\n",
    "        to_utm_east.transform(180, lat_min),  # Close the polygon\n",
    "    ]\n",
    "    east_polygon = shapely.geometry.Polygon(east_poly_coords)\n",
    "\n",
    "    # Define west buffer polygon in UTM coordinates\n",
    "    west_poly_coords = [\n",
    "        to_utm_west.transform(-180, lat_min),  # Start at (-180째, lat_min)\n",
    "        to_utm_west.transform(-180 - buffer_size_meters / 111320, lat_min),  # West edge\n",
    "        to_utm_west.transform(-180 - buffer_size_meters / 111320, lat_max),  # West edge\n",
    "        to_utm_west.transform(-180, lat_max),  # Back at (-180째, lat_max)\n",
    "        to_utm_west.transform(-180, lat_min),  # Close the polygon\n",
    "    ]\n",
    "    west_polygon = shapely.geometry.Polygon(west_poly_coords)\n",
    "\n",
    "    # Convert polygons back to EPSG:4326\n",
    "    east_polygon_epsg4326 = shapely.ops.transform(to_epsg4326.transform, east_polygon)\n",
    "    west_polygon_epsg4326 = shapely.ops.transform(to_epsg4326.transform, west_polygon)\n",
    "\n",
    "    # Combine both polygons into a GeoDataFrame\n",
    "    combined_polygons = gpd.GeoDataFrame(\n",
    "        geometry=[east_polygon_epsg4326, west_polygon_epsg4326], crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    return combined_polygons\n",
    "\n",
    "\n",
    "# Example usage\n",
    "buffer_size = 300000  # Buffer size in meters\n",
    "antimeridian_buffer = buffer_antimeridian(buffer_size)\n",
    "print(antimeridian_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "antimeridian_buffer.iloc[[1]].explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tiles.sample(1).explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import antimeridian\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import mapping, shape\n",
    "\n",
    "\n",
    "def fix_geom(geom):\n",
    "    \"\"\"\n",
    "    Fix geometries using the antimeridian library.\n",
    "\n",
    "    Args:\n",
    "        geom (shapely.geometry.base.BaseGeometry): Input geometry.\n",
    "\n",
    "    Returns:\n",
    "        shapely.geometry.base.BaseGeometry: Fixed geometry or the original geometry if it can't be fixed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fix geometry using antimeridian library\n",
    "        if geom.is_empty:\n",
    "            return geom  # Skip empty geometries\n",
    "        elif geom.is_valid:\n",
    "            return geom  # Return if already valid\n",
    "\n",
    "        # Try fixing specific geometry types\n",
    "        if geom.geom_type == \"Polygon\":\n",
    "            return antimeridian.fix_polygon(geom)\n",
    "        elif geom.geom_type == \"MultiPolygon\":\n",
    "            return antimeridian.fix_multipolygon(geom)\n",
    "        else:\n",
    "            # Attempt general fix for other shapes\n",
    "            return antimeridian.fix_shape(geom)\n",
    "    except Exception as e:\n",
    "        # Log an issue with fixing and return the original geometry\n",
    "        print(f\"Could not fix geometry: {e}\")\n",
    "        return geom\n",
    "\n",
    "\n",
    "def fix_invalid_geometries(gdf):\n",
    "    \"\"\"\n",
    "    Fix invalid geometries in a GeoDataFrame.\n",
    "\n",
    "    Args:\n",
    "        gdf (geopandas.GeoDataFrame): Input GeoDataFrame.\n",
    "\n",
    "    Returns:\n",
    "        geopandas.GeoDataFrame: GeoDataFrame with fixed geometries.\n",
    "    \"\"\"\n",
    "    # Identify invalid geometries\n",
    "    invalid_mask = ~gdf.is_valid\n",
    "    invalid_geometries = gdf.loc[invalid_mask]\n",
    "\n",
    "    print(f\"Found {len(invalid_geometries)} invalid geometries.\")\n",
    "\n",
    "    # Apply the fix_geom function to invalid geometries\n",
    "    gdf.loc[invalid_mask, \"geometry\"] = invalid_geometries.geometry.map(fix_geom)\n",
    "\n",
    "    # Re-check if geometries are now valid\n",
    "    still_invalid = gdf[~gdf.is_valid]\n",
    "    if len(still_invalid) > 0:\n",
    "        print(f\"Still invalid geometries: {len(still_invalid)}\")\n",
    "    else:\n",
    "        print(\"All geometries are now valid.\")\n",
    "\n",
    "    return gdf\n",
    "\n",
    "\n",
    "buffer = fix_invalid_geometries(coastline_buffer)\n",
    "buffer = buffer[buffer.is_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coastpy.io.utils import name_data\n",
    "\n",
    "name_data(\n",
    "    buffer, include_random_hex=False, filename_prefix=\"osm_coastline_buffer_2000m\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 0.01  # Adjust this value based on your acceptable resolution loss\n",
    "simplified_buffer = buffer.copy()\n",
    "simplified_buffer[\"geometry\"] = buffer.geometry.simplify(\n",
    "    tolerance, preserve_topology=True\n",
    ")\n",
    "\n",
    "# Save the simplified buffer\n",
    "simplified_buffer.to_parquet(\"/Users/calkoen/tmp/buffer_simplified.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplified_buffer.to_file(\"/Users/calkoen/tmp/buffer_simplified.gpkg\")\n",
    "buffer.to_file(\"/Users/calkoen/tmp/buffer.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "invals = buffer[~buffer.is_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.to_parquet(\"/Users/calkoen/tmp/buffer.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "invals.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import antimeridian\n",
    "import shapely\n",
    "\n",
    "\n",
    "def fix_geom(geom):\n",
    "    try:\n",
    "        if geom.dtype == \n",
    "        geom = antimeridian.fix_shape(geom)\n",
    "    except:\n",
    "        print(\"not fixedj\")\n",
    "        return geom\n",
    "        \n",
    "\n",
    "invalids.geometry.map(fix_geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalids.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geodatasets\n",
    "\n",
    "geodatasets.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.read_file(gpd.datasets.get_path(\"natearth_lowres\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: LOAD DATA\n",
    "s2_tiles = retrieve_s2_tiles().to_crs(4326)\n",
    "rois = retrieve_rois().to_crs(4326)\n",
    "# TODO: write STAC catalog for the coastal buffer\n",
    "buffer = dask_geopandas.read_parquet(\n",
    "    \"az://coastline-buffer/osm-coastlines-buffer-2000m.parquet\",\n",
    "    storage_options=storage_options,\n",
    ").compute()\n",
    "quadtiles = make_mercantiles(zoom_level=MERCANTILES_ZOOM_LEVEL).to_crs(4326)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    classifier = retrieve_coastsat_classifier()\n",
    "\n",
    "region_of_interest = infer_region_of_interest(ROI)\n",
    "\n",
    "# NOTE: make an overlay with a coastline buffer to avoid querying data we do not need\n",
    "buffer_aoi = gpd.overlay(buffer, region_of_interest[[\"geometry\"]].to_crs(buffer.crs))\n",
    "\n",
    "# TODO: add heuristic to decide which s2 tiles to use\n",
    "s2_tilenames_to_process = gpd.sjoin(\n",
    "    s2_tiles, buffer_aoi.to_crs(s2_tiles.crs)\n",
    ").Name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "west, south, east, north = m.west, m.south, m.east, m.north\n",
    "# Note: small little hack to ensure the notebook also works when running all cells at once\n",
    "if not west:\n",
    "    west, south, east, north = (\n",
    "        -1.4987754821777346,\n",
    "        46.328320550966765,\n",
    "        -1.446976661682129,\n",
    "        46.352022707044455,\n",
    "    )\n",
    "roi = gpd.GeoDataFrame(\n",
    "    geometry=[shapely.geometry.box(west, south, east, north)], crs=4326\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import planetary_computer as pc\n",
    "\n",
    "from coastpy.eo.collection import (\n",
    "    CopernicusDEMCollection,\n",
    "    DeltaDTMCollection,\n",
    "    S2Collection,\n",
    ")\n",
    "\n",
    "s2_ = (\n",
    "    S2Collection()\n",
    "    .search(\n",
    "        roi,\n",
    "        datetime_range=\"2023-05-01/2023-07-31\",\n",
    "        query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    "    )\n",
    "    .load(\n",
    "        bands=[\"blue\", \"green\", \"red\", \"nir\", \"swir16\"],\n",
    "        # composite=50,\n",
    "        spectral_indices=[\"NDWI\", \"NDVI\"],\n",
    "        chunks={\"x\": 256, \"y\": 256},\n",
    "        patch_url=pc.sign,\n",
    "    )\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "deltadtm = DeltaDTMCollection().search(roi).load().execute()\n",
    "cop_dem = CopernicusDEMCollection().search(roi).load(patch_url=pc.sign).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = s2_.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_composite(ds):\n",
    "    \"\"\"\n",
    "    Compute the median composite along the 'time' dimension.\n",
    "\n",
    "    Args:\n",
    "        ds (xr.Dataset): Input dataset with dimensions (time, y, x).\n",
    "\n",
    "    Returns:\n",
    "        xr.Dataset: Median composite with dimensions (y, x).\n",
    "    \"\"\"\n",
    "    composite = xr.apply_ufunc(\n",
    "        np.nanmedian,\n",
    "        ds,  # Input dataset\n",
    "        input_core_dims=[[\"time\"]],  # Operate along \"time\" dimension\n",
    "        output_core_dims=[[]],  # Collapsed \"time\" dimension\n",
    "        vectorize=True,  # Vectorize to apply along (y, x)\n",
    "        dask=\"parallelized\",  # Support for Dask arrays\n",
    "        output_dtypes=[np.float32],  # Match input dtype\n",
    "    )\n",
    "    return composite\n",
    "\n",
    "\n",
    "composite = median_composite(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = s2.median(\"time\", skipna=True, keep_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.quantile(0.15, dim=\"time\", skipna=True, keep_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "def benchmark(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function to benchmark a given function.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return result, elapsed_time\n",
    "\n",
    "\n",
    "def median_composite_builtin(ds):\n",
    "    \"\"\"\n",
    "    Compute the median composite along the 'time' dimension using Xarray's built-in median.\n",
    "    \"\"\"\n",
    "    return ds.median(\"time\", skipna=True, keep_attrs=True)\n",
    "\n",
    "\n",
    "def median_composite_ufunc(ds):\n",
    "    \"\"\"\n",
    "    Compute the median composite along the 'time' dimension using NumPy's nanmedian and Xarray ufunc.\n",
    "    \"\"\"\n",
    "    composite = xr.apply_ufunc(\n",
    "        np.nanmedian,\n",
    "        ds,\n",
    "        input_core_dims=[[\"time\"]],\n",
    "        output_core_dims=[[]],\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "        output_dtypes=[np.float32],\n",
    "    )\n",
    "    return composite\n",
    "\n",
    "\n",
    "def quantile_composite_builtin(ds, quantile):\n",
    "    \"\"\"\n",
    "    Compute the quantile composite along the 'time' dimension using Xarray's built-in quantile.\n",
    "    \"\"\"\n",
    "    return ds.quantile(quantile, dim=\"time\", skipna=True, keep_attrs=True)\n",
    "\n",
    "\n",
    "def quantile_composite_ufunc(ds, quantile):\n",
    "    \"\"\"\n",
    "    Compute the quantile composite along the 'time' dimension using NumPy's nanpercentile and Xarray ufunc.\n",
    "    \"\"\"\n",
    "    composite = xr.apply_ufunc(\n",
    "        np.nanpercentile,\n",
    "        ds,\n",
    "        kwargs={\"q\": quantile * 100},\n",
    "        input_core_dims=[[\"time\"]],\n",
    "        output_core_dims=[[]],\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "        output_dtypes=[np.float32],\n",
    "    )\n",
    "    return composite\n",
    "\n",
    "\n",
    "def compute_percentage_difference(array1, array2):\n",
    "    \"\"\"\n",
    "    Compute the percentage difference between two arrays.\n",
    "    \"\"\"\n",
    "    diff = np.abs(array1 - array2)\n",
    "    return np.mean(diff / (np.abs(array1) + np.abs(array2) + 1e-10)) * 100\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Simulated dataset for testing\n",
    "    time = pd.date_range(\"2023-01-01\", \"2023-01-10\")\n",
    "    y = np.linspace(-10, 10, 50)\n",
    "    x = np.linspace(-10, 10, 50)\n",
    "    data = np.random.rand(len(time), len(y), len(x))\n",
    "\n",
    "    ds = xr.DataArray(\n",
    "        data,\n",
    "        coords={\"time\": time, \"y\": y, \"x\": x},\n",
    "        dims=(\"time\", \"y\", \"x\"),\n",
    "        name=\"test_data\",\n",
    "    )\n",
    "\n",
    "    # Quantile to use for quantile composite tests\n",
    "    quantile = 0.15\n",
    "\n",
    "    # Benchmark each method\n",
    "    methods = [\n",
    "        (\"Median (Xarray Built-in)\", median_composite_builtin, {\"ds\": ds}),\n",
    "        (\"Median (Xarray Ufunc)\", median_composite_ufunc, {\"ds\": ds}),\n",
    "        (\n",
    "            \"Quantile (Xarray Built-in)\",\n",
    "            quantile_composite_builtin,\n",
    "            {\"ds\": ds, \"quantile\": quantile},\n",
    "        ),\n",
    "        (\n",
    "            \"Quantile (Xarray Ufunc)\",\n",
    "            quantile_composite_ufunc,\n",
    "            {\"ds\": ds, \"quantile\": quantile},\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    outputs = {}\n",
    "\n",
    "    for method_name, method, kwargs in methods:\n",
    "        result, elapsed_time = benchmark(method, **kwargs)\n",
    "        outputs[method_name] = result\n",
    "        results.append({\"Method\": method_name, \"Time (s)\": elapsed_time})\n",
    "\n",
    "    # Compare results and calculate percentage differences\n",
    "    comparisons = [\n",
    "        (\"Median (Xarray Built-in)\", \"Median (Xarray Ufunc)\"),\n",
    "        (\"Quantile (Xarray Built-in)\", \"Quantile (Xarray Ufunc)\"),\n",
    "    ]\n",
    "\n",
    "    for method1, method2 in comparisons:\n",
    "        diff = compute_percentage_difference(\n",
    "            outputs[method1].values, outputs[method2].values\n",
    "        )\n",
    "        results.append(\n",
    "            {\n",
    "                \"Method\": f\"Comparison {method1} vs {method2}\",\n",
    "                \"Time (s)\": f\"Diff: {diff:.6f}%\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Display results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.median(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "def percentile_composite(ds, q, nodata=np.nan):\n",
    "    \"\"\"\n",
    "    Create a percentile composite from a datacube.\n",
    "\n",
    "    Args:\n",
    "        ds (xr.Dataset): Input Dataset with dimensions (time, y, x).\n",
    "        q (float): Percentile to compute (e.g., 15 for 15th percentile).\n",
    "        nodata (float, optional): Value representing nodata. Default is NaN.\n",
    "\n",
    "    Returns:\n",
    "        xr.Dataset: Composite with dimensions (y, x).\n",
    "    \"\"\"\n",
    "    # Mask nodata values\n",
    "    ds_masked = ds.where(ds != nodata)\n",
    "\n",
    "    # Apply percentile calculation\n",
    "    def nanpercentile(data, axis, q):\n",
    "        return np.nanpercentile(data, q=q, axis=axis)\n",
    "\n",
    "    composite = xr.apply_ufunc(\n",
    "        nanpercentile,\n",
    "        ds_masked,\n",
    "        input_core_dims=[[\"time\"]],  # Operate along the \"time\" dimension\n",
    "        output_core_dims=[[]],  # Output has no \"time\" dimension\n",
    "        kwargs={\"q\": q},  # Percentile value\n",
    "        dask=\"parallelized\",  # Enable Dask support\n",
    "        output_dtypes=[float],  # Output type\n",
    "        keep_attrs=True,  # Retain attributes\n",
    "    )\n",
    "\n",
    "    # Preserve nodata value in output\n",
    "    for var in composite.data_vars:\n",
    "        composite[var].attrs[\"nodata\"] = nodata\n",
    "\n",
    "    return composite\n",
    "\n",
    "\n",
    "percentile_composite(xx, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# Sample dataset for testing\n",
    "xx = xr.Dataset(\n",
    "    {\n",
    "        \"nir\": ([\"time\", \"y\", \"x\"], np.random.random((10, 100, 100))),\n",
    "        \"green\": ([\"time\", \"y\", \"x\"], np.random.random((10, 100, 100))),\n",
    "    },\n",
    "    coords={\n",
    "        \"time\": np.arange(10),\n",
    "        \"y\": np.linspace(0, 100, 100),\n",
    "        \"x\": np.linspace(0, 100, 100),\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "def median_composite(ds):\n",
    "    \"\"\"\n",
    "    Compute a median composite over the 'time' dimension.\n",
    "\n",
    "    Args:\n",
    "        ds (xr.Dataset): Input Dataset with dimensions (time, y, x).\n",
    "\n",
    "    Returns:\n",
    "        xr.Dataset: Composite with dimensions (y, x).\n",
    "    \"\"\"\n",
    "    # Apply median calculation using xr.apply_ufunc\n",
    "    composite = xr.apply_ufunc(\n",
    "        np.nanmedian,  # Use numpy's nanmedian\n",
    "        ds,  # Input dataset\n",
    "        input_core_dims=[[\"time\"]],  # Operate along the \"time\" dimension\n",
    "        output_core_dims=[[]],  # Result has no \"time\" dimension\n",
    "        dask=\"parallelized\",  # Support for Dask arrays\n",
    "        output_dtypes=[ds[\"nir\"].dtype],  # Output dtype inferred from input\n",
    "        keep_attrs=True,  # Retain attributes from the input dataset\n",
    "    )\n",
    "    return composite\n",
    "\n",
    "\n",
    "# Test the function\n",
    "composite = median_composite(xx)\n",
    "print(composite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.apply_ufunc(\n",
    "    np.nanpercentile,  # Handle NaNs during computation\n",
    "    xx,  # Dataset to process\n",
    "    input_core_dims=[[\"time\"]],  # Specify \"time\" as the axis for computation\n",
    "    output_core_dims=[[\"y\", \"x\"]],  # Output should retain \"y\" and \"x\" dimensions\n",
    "    kwargs={\"q\": 15},  # Pass the percentile to compute\n",
    "    # dask=\"parallelized\",  # Enable Dask support\n",
    "    # output_dtypes=[float],  # Define the output data type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "def composite(ds: xr.Dataset, percentile: float, nodata: float = np.nan) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Create a percentile composite from a datacube.\n",
    "\n",
    "    Args:\n",
    "        ds (xr.Dataset): The input datacube with dimensions (\"time\", \"y\", \"x\").\n",
    "        percentile (float): The percentile to compute (e.g., 15 for 15th percentile).\n",
    "        nodata (float): The value representing nodata. Default is NaN.\n",
    "\n",
    "    Returns:\n",
    "        xr.Dataset: The composite with the specified percentile.\n",
    "    \"\"\"\n",
    "    # 1. Mask nodata values\n",
    "    ds_masked = ds.where(ds != nodata)\n",
    "\n",
    "    # 2. Apply percentile calculation\n",
    "    composite = xr.apply_ufunc(\n",
    "        np.nanpercentile,  # Handle NaNs during computation\n",
    "        ds_masked,  # Dataset to process\n",
    "        input_core_dims=[[\"time\"]],  # Specify \"time\" as the axis for computation\n",
    "        output_core_dims=[[\"y\", \"x\"]],  # Output should retain \"y\" and \"x\" dimensions\n",
    "        kwargs={\"q\": percentile},  # Pass the percentile to compute\n",
    "        dask=\"parallelized\",  # Enable Dask support\n",
    "        output_dtypes=[float],  # Define the output data type\n",
    "    )\n",
    "\n",
    "    # 3. Preserve nodata values\n",
    "    for var in ds.data_vars:\n",
    "        composite[var] = composite[var].where(~composite[var].isnull(), nodata)\n",
    "        composite[var].attrs[\"nodata\"] = nodata\n",
    "        composite[var] = composite[var].rio.write_nodata(nodata)\n",
    "\n",
    "    # 4. Return the composite\n",
    "    return composite\n",
    "\n",
    "\n",
    "composite(s2, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "agg = s2.reduce(np.percentile, dim=\"time\", q=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.nir.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.isel(time=10).blue.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.nir.hvplot(x=\"x\", y=\"y\", geo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.nirhvplot(x=\"x\", y=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cop_dem[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cop_dem = cop_dem.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltadtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coastal-full] *",
   "language": "python",
   "name": "conda-env-coastal-full-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
